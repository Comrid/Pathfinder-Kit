"""
qlearning_line_tracker.py - Q-Learning ê¸°ë°˜ ë¼ì¸ íŠ¸ë ˆì´ì‹±
ì¹´ë©”ë¼ë¥¼ ì´ìš©í•œ ë¼ì¸ ì¶”ì  í•™ìŠµ ì‹œìŠ¤í…œ
"""

import time
import random
import numpy as np
import json
import os
import cv2
from typing import List, Dict, Tuple, Optional
import sys
from pathlib import Path

# ëª¨í„° ì»¨íŠ¸ë¡¤ëŸ¬ import
sys.path.append(str(Path(__file__).parent.parent))
from MotorClassTest.Motor import MotorController

class LineTrackingQLearning:
    """Q-Learning ê¸°ë°˜ ë¼ì¸ íŠ¸ë ˆì´ì‹± ì—ì´ì „íŠ¸"""
    
    def __init__(self, config: Optional[dict] = None):
        """
        Q-Learning ë¼ì¸ íŠ¸ë ˆì´ì‹± ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        
        Args:
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬ (ì„ íƒì‚¬í•­)
        """
        # í•˜ë“œì›¨ì–´ ì´ˆê¸°í™”
        self.motor = MotorController()
        self.camera = cv2.VideoCapture(0)
        self.camera.set(cv2.CAP_PROP_FRAME_WIDTH, 320)
        self.camera.set(cv2.CAP_PROP_FRAME_HEIGHT, 240)
        self.camera.set(cv2.CAP_PROP_FPS, 30)
        
        # ì„¤ì • ë¡œë“œ
        self.config = self._load_config(config)
        
        # Q-Learning ë³€ìˆ˜
        self.q_table: Dict[str, List[float]] = {}
        self.current_state = ""
        self.last_action = 0
        self.episode = 0
        self.steps = 0
        self.total_reward = 0
        self.running = False
        
        # í†µê³„ ë³€ìˆ˜
        self.episode_rewards = []
        self.episode_steps = []
        
        # ë¼ì¸ íŠ¸ë ˆì´ì‹± ë³€ìˆ˜
        self.line_center = 160  # í™”ë©´ ì¤‘ì•™ (320/2)
        self.last_line_position = self.line_center
        self.line_lost_count = 0
        
        # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.config['models_dir'], exist_ok=True)
        
        print("ğŸ¤– Q-Learning ë¼ì¸ íŠ¸ë ˆì´ì‹± ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ!")
        print(f"ì•¡ì…˜ ìˆ˜: {len(self.config['actions'])}")
        print(f"ìƒíƒœ ê³µê°„: {self.config['line_bins']} bins")
    
    def _load_config(self, config: Optional[dict]) -> dict:
        """ì„¤ì • ë¡œë“œ ë° ê²€ì¦"""
        default_config = {
            # Q-Learning íŒŒë¼ë¯¸í„°
            'learning_rate': 0.15,       # í•™ìŠµë¥  (alpha)
            'discount_factor': 0.9,      # í• ì¸ ì¸ìˆ˜ (gamma)
            'exploration_rate': 1.0,     # ì´ˆê¸° íƒí—˜ë¥  (epsilon)
            'exploration_min': 0.05,     # ìµœì†Œ íƒí—˜ë¥ 
            'exploration_decay': 0.998,  # íƒí—˜ë¥  ê°ì†Œìœ¨
            
            # ë¼ì¸ íŠ¸ë ˆì´ì‹± ì„¤ì •
            'line_bins': 7,              # ë¼ì¸ ìœ„ì¹˜ êµ¬ê°„ ìˆ˜
            'line_threshold': 50,        # ë¼ì¸ ê²€ì¶œ ì„ê³„ê°’
            'roi_height': 80,            # ROI ë†’ì´
            'roi_y_offset': 160,         # ROI Y ì‹œì‘ì 
            'min_line_area': 100,        # ìµœì†Œ ë¼ì¸ ë©´ì 
            
            # ëª¨í„° ì„¤ì •
            'base_speed': 35,            # ê¸°ë³¸ ì†ë„
            'turn_speed': 50,            # íšŒì „ ì†ë„
            'slow_speed': 20,            # ëŠë¦° ì†ë„
            
            # ì•¡ì…˜ ì •ì˜ (left_speed, right_speed)
            'actions': [
                (35, 35),    # 0: ì§ì§„
                (20, 35),    # 1: ì•½ê°„ ì¢ŒíšŒì „
                (35, 20),    # 2: ì•½ê°„ ìš°íšŒì „
                (10, 50),    # 3: ê°•í•œ ì¢ŒíšŒì „
                (50, 10),    # 4: ê°•í•œ ìš°íšŒì „
                (0, 35),     # 5: ì œìë¦¬ ì¢ŒíšŒì „
                (35, 0),     # 6: ì œìë¦¬ ìš°íšŒì „
                (15, 15),    # 7: ëŠë¦° ì§ì§„
            ],
            
            # ë³´ìƒ ì„¤ì •
            'rewards': {
                'on_line': 10,           # ë¼ì¸ ìœ„ì— ìˆì„ ë•Œ
                'near_line': 5,          # ë¼ì¸ ê·¼ì²˜ì— ìˆì„ ë•Œ
                'off_line': -5,          # ë¼ì¸ì—ì„œ ë²—ì–´ë‚¬ì„ ë•Œ
                'line_lost': -20,        # ë¼ì¸ì„ ì™„ì „íˆ ë†“ì³¤ì„ ë•Œ
                'center_bonus': 15,      # ì¤‘ì•™ì— ìˆì„ ë•Œ ë³´ë„ˆìŠ¤
                'step_penalty': -0.5,    # ìŠ¤í… í˜ë„í‹°
            },
            
            # íŒŒì¼ ì„¤ì •
            'models_dir': 'Q_LineTracking/models',
            'model_prefix': 'line_qtable_',
            'save_interval': 25,         # ëª¨ë¸ ì €ì¥ ê°„ê²©
        }
        
        if config:
            default_config.update(config)
        
        return default_config
    
    def capture_frame(self) -> Optional[np.ndarray]:
        """ì¹´ë©”ë¼ì—ì„œ í”„ë ˆì„ ìº¡ì²˜"""
        try:
            ret, frame = self.camera.read()
            if ret:
                return frame
            return None
        except Exception as e:
            print(f"ì¹´ë©”ë¼ ìº¡ì²˜ ì˜¤ë¥˜: {e}")
            return None
    
    def process_frame(self, frame: np.ndarray) -> Tuple[int, bool]:
        """
        í”„ë ˆì„ ì²˜ë¦¬í•˜ì—¬ ë¼ì¸ ìœ„ì¹˜ ê²€ì¶œ
        
        Args:
            frame: ì…ë ¥ í”„ë ˆì„
            
        Returns:
            Tuple[int, bool]: (ë¼ì¸ ìœ„ì¹˜, ë¼ì¸ ê²€ì¶œ ì—¬ë¶€)
        """
        try:
            # ROI ì„¤ì •
            height, width = frame.shape[:2]
            roi_y = self.config['roi_y_offset']
            roi_height = self.config['roi_height']
            roi = frame[roi_y:roi_y + roi_height, :]
            
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
            
            # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬
            blurred = cv2.GaussianBlur(gray, (5, 5), 0)
            
            # ì´ì§„í™” (ê²€ì€ìƒ‰ ë¼ì¸ ê²€ì¶œ)
            _, binary = cv2.threshold(blurred, self.config['line_threshold'], 255, cv2.THRESH_BINARY_INV)
            
            # ëª¨í´ë¡œì§€ ì—°ì‚°ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì œê±°
            kernel = np.ones((3, 3), np.uint8)
            binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
            binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)
            
            # ìœ¤ê³½ì„  ê²€ì¶œ
            contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            if contours:
                # ê°€ì¥ í° ìœ¤ê³½ì„  ì„ íƒ
                largest_contour = max(contours, key=cv2.contourArea)
                area = cv2.contourArea(largest_contour)
                
                if area > self.config['min_line_area']:
                    # ìœ¤ê³½ì„ ì˜ ì¤‘ì‹¬ì  ê³„ì‚°
                    M = cv2.moments(largest_contour)
                    if M["m00"] != 0:
                        cx = int(M["m10"] / M["m00"])
                        self.last_line_position = cx
                        return cx, True
            
            # ë¼ì¸ì„ ì°¾ì§€ ëª»í•œ ê²½ìš°
            return self.last_line_position, False
            
        except Exception as e:
            print(f"í”„ë ˆì„ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return self.last_line_position, False
    
    def discretize_line_position(self, line_x: int, line_detected: bool) -> int:
        """ë¼ì¸ ìœ„ì¹˜ë¥¼ ì´ì‚°í™”ëœ ìƒíƒœë¡œ ë³€í™˜"""
        if not line_detected:
            return self.config['line_bins']  # ë¼ì¸ ì—†ìŒ ìƒíƒœ
        
        # í™”ë©´ì„ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
        width = 320  # ì¹´ë©”ë¼ í•´ìƒë„
        bin_size = width / self.config['line_bins']
        
        bin_index = min(int(line_x / bin_size), self.config['line_bins'] - 1)
        return bin_index
    
    def get_state(self) -> str:
        """í˜„ì¬ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°"""
        frame = self.capture_frame()
        if frame is None:
            return f"line{self.config['line_bins']}"  # ì¹´ë©”ë¼ ì˜¤ë¥˜ ìƒíƒœ
        
        line_x, line_detected = self.process_frame(frame)
        line_bin = self.discretize_line_position(line_x, line_detected)
        
        # ë¼ì¸ ì†ì‹¤ ì¹´ìš´íŠ¸ ì¶”ê°€
        if not line_detected:
            self.line_lost_count += 1
        else:
            self.line_lost_count = 0
        
        # ìƒíƒœë¥¼ ë¬¸ìì—´ë¡œ í‘œí˜„
        state = f"line{line_bin}"
        if self.line_lost_count > 5:
            state += "_lost"
        
        return state
    
    def choose_action(self, state: str) -> int:
        """Îµ-greedy ì •ì±…ìœ¼ë¡œ ì•¡ì…˜ ì„ íƒ"""
        # ìƒíƒœê°€ Q-tableì— ì—†ìœ¼ë©´ ì´ˆê¸°í™”
        if state not in self.q_table:
            self.q_table[state] = [0.0] * len(self.config['actions'])
        
        # íƒí—˜ vs í™œìš©
        if random.random() < self.config['exploration_rate']:
            # íƒí—˜: ëœë¤ ì•¡ì…˜
            return random.randint(0, len(self.config['actions']) - 1)
        else:
            # í™œìš©: ìµœê³  Qê°’ ì•¡ì…˜
            return int(np.argmax(self.q_table[state]))
    
    def execute_action(self, action_idx: int) -> None:
        """ì•¡ì…˜ ì‹¤í–‰"""
        left_speed, right_speed = self.config['actions'][action_idx]
        
        try:
            self.motor.set_individual_speeds(right_speed, left_speed)
        except Exception as e:
            print(f"ì•¡ì…˜ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            self.motor.stop_motors()
    
    def get_reward(self, line_x: int, line_detected: bool, action_idx: int) -> float:
        """ë³´ìƒ ê³„ì‚°"""
        rewards = self.config['rewards']
        
        if not line_detected:
            return rewards['line_lost']
        
        # ë¼ì¸ ì¤‘ì•™ìœ¼ë¡œë¶€í„°ì˜ ê±°ë¦¬ ê³„ì‚°
        center_distance = abs(line_x - self.line_center)
        
        # ê±°ë¦¬ ê¸°ë°˜ ë³´ìƒ
        if center_distance < 20:  # ì¤‘ì•™ ê·¼ì²˜
            reward = rewards['center_bonus']
        elif center_distance < 50:  # ë¼ì¸ ìœ„
            reward = rewards['on_line']
        elif center_distance < 80:  # ë¼ì¸ ê·¼ì²˜
            reward = rewards['near_line']
        else:  # ë¼ì¸ì—ì„œ ë©€ë¦¬
            reward = rewards['off_line']
        
        # ìŠ¤í… í˜ë„í‹° ì¶”ê°€
        reward += rewards['step_penalty']
        
        return reward
    
    def update_q_value(self, state: str, action: int, reward: float, next_state: str) -> None:
        """Q-value ì—…ë°ì´íŠ¸ (Q-learning ê³µì‹)"""
        # ìƒíƒœê°€ Q-tableì— ì—†ìœ¼ë©´ ì´ˆê¸°í™”
        if state not in self.q_table:
            self.q_table[state] = [0.0] * len(self.config['actions'])
        if next_state not in self.q_table:
            self.q_table[next_state] = [0.0] * len(self.config['actions'])
        
        # Q-learning ì—…ë°ì´íŠ¸ ê³µì‹
        best_next_action = np.argmax(self.q_table[next_state])
        td_target = reward + self.config['discount_factor'] * self.q_table[next_state][best_next_action]
        td_error = td_target - self.q_table[state][action]
        self.q_table[state][action] += self.config['learning_rate'] * td_error
    
    def decay_exploration(self) -> None:
        """íƒí—˜ë¥  ê°ì†Œ"""
        self.config['exploration_rate'] = max(
            self.config['exploration_min'],
            self.config['exploration_rate'] * self.config['exploration_decay']
        )
    
    def train_episode(self, max_steps: int = 1000) -> float:
        """í•œ ì—í”¼ì†Œë“œ í›ˆë ¨"""
        self.episode += 1
        self.steps = 0
        episode_reward = 0
        
        print(f"\nğŸ¯ ì—í”¼ì†Œë“œ {self.episode} ì‹œì‘")
        
        # ì´ˆê¸° ìƒíƒœ
        self.current_state = self.get_state()
        
        for step in range(max_steps):
            if not self.running:
                break
            
            # ì•¡ì…˜ ì„ íƒ ë° ì‹¤í–‰
            action = self.choose_action(self.current_state)
            self.execute_action(action)
            
            # ì ì‹œ ëŒ€ê¸° (ì•¡ì…˜ ì‹¤í–‰ ì‹œê°„)
            time.sleep(0.1)
            
            # ë‹¤ìŒ ìƒíƒœ ë° ë³´ìƒ ê´€ì°°
            frame = self.capture_frame()
            if frame is not None:
                line_x, line_detected = self.process_frame(frame)
                next_state = self.get_state()
                reward = self.get_reward(line_x, line_detected, action)
                
                # Q-value ì—…ë°ì´íŠ¸
                self.update_q_value(self.current_state, action, reward, next_state)
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                episode_reward += reward
                self.steps += 1
                
                # ì§„í–‰ ìƒí™© ì¶œë ¥
                if step % 50 == 0:
                    action_name = self._get_action_name(action)
                    print(f"  ìŠ¤í… {step}: ë¼ì¸ìœ„ì¹˜={line_x}, ê²€ì¶œ={line_detected}, "
                          f"ì•¡ì…˜={action_name}, ë³´ìƒ={reward:.1f}, Îµ={self.config['exploration_rate']:.3f}")
                
                # ìƒíƒœ ì—…ë°ì´íŠ¸
                self.current_state = next_state
            else:
                # ì¹´ë©”ë¼ ì˜¤ë¥˜ ì‹œ ì—í”¼ì†Œë“œ ì¢…ë£Œ
                print("  âš ï¸ ì¹´ë©”ë¼ ì˜¤ë¥˜! ì—í”¼ì†Œë“œ ì¢…ë£Œ")
                break
        
        # ì—í”¼ì†Œë“œ ì¢…ë£Œ
        self.motor.stop_motors()
        self.decay_exploration()
        
        # í†µê³„ ì €ì¥
        self.episode_rewards.append(episode_reward)
        self.episode_steps.append(self.steps)
        
        print(f"âœ… ì—í”¼ì†Œë“œ {self.episode} ì™„ë£Œ: ë³´ìƒ={episode_reward:.1f}, ìŠ¤í…={self.steps}")
        
        return episode_reward
    
    def _get_action_name(self, action_idx: int) -> str:
        """ì•¡ì…˜ ì¸ë±ìŠ¤ë¥¼ ì´ë¦„ìœ¼ë¡œ ë³€í™˜"""
        action_names = [
            "ì§ì§„", "ì•½ê°„ì¢Œ", "ì•½ê°„ìš°", "ê°•í•œì¢Œ", "ê°•í•œìš°", "ì œìë¦¬ì¢Œ", "ì œìë¦¬ìš°", "ëŠë¦°ì§ì§„"
        ]
        return action_names[action_idx] if action_idx < len(action_names) else f"ì•¡ì…˜{action_idx}"
    
    def start_training(self, episodes: int = 500) -> None:
        """í›ˆë ¨ ì‹œì‘"""
        print(f"ğŸš€ Q-Learning ë¼ì¸ íŠ¸ë ˆì´ì‹± í›ˆë ¨ ì‹œì‘! ({episodes} ì—í”¼ì†Œë“œ)")
        print("Ctrl+Cë¡œ ì¤‘ë‹¨ ê°€ëŠ¥")
        
        self.running = True
        
        try:
            for episode in range(episodes):
                if not self.running:
                    break
                
                # ì—í”¼ì†Œë“œ í›ˆë ¨
                episode_reward = self.train_episode()
                
                # ì£¼ê¸°ì ìœ¼ë¡œ ëª¨ë¸ ì €ì¥
                if episode % self.config['save_interval'] == 0:
                    self.save_model()
                
                # í†µê³„ ì¶œë ¥
                if episode % 10 == 0:
                    avg_reward = np.mean(self.episode_rewards[-10:])
                    avg_steps = np.mean(self.episode_steps[-10:])
                    print(f"\nğŸ“Š ìµœê·¼ 10 ì—í”¼ì†Œë“œ í‰ê· : ë³´ìƒ={avg_reward:.1f}, ìŠ¤í…={avg_steps:.1f}")
                    print(f"Q-table í¬ê¸°: {len(self.q_table)} ìƒíƒœ")
                
                # ì—í”¼ì†Œë“œ ê°„ íœ´ì‹
                time.sleep(0.5)
                
        except KeyboardInterrupt:
            print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ í›ˆë ¨ ì¤‘ë‹¨")
        
        finally:
            self.stop_training()
    
    def stop_training(self) -> None:
        """í›ˆë ¨ ì¤‘ë‹¨"""
        self.running = False
        self.motor.stop_motors()
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        final_path = self.save_model("final_line_model.json")
        print(f"ğŸ‰ í›ˆë ¨ ì™„ë£Œ! ìµœì¢… ëª¨ë¸ ì €ì¥: {final_path}")
        
        # í†µê³„ ì¶œë ¥
        if self.episode_rewards:
            print(f"\nğŸ“ˆ í›ˆë ¨ í†µê³„:")
            print(f"  ì´ ì—í”¼ì†Œë“œ: {len(self.episode_rewards)}")
            print(f"  í‰ê·  ë³´ìƒ: {np.mean(self.episode_rewards):.2f}")
            print(f"  ìµœê³  ë³´ìƒ: {np.max(self.episode_rewards):.2f}")
            print(f"  í‰ê·  ìŠ¤í…: {np.mean(self.episode_steps):.1f}")
            print(f"  Q-table í¬ê¸°: {len(self.q_table)} ìƒíƒœ")
    
    def save_model(self, filename: Optional[str] = None) -> str:
        """ëª¨ë¸ ì €ì¥"""
        if filename is None:
            filename = f"{self.config['model_prefix']}{self.episode:06d}.json"
        
        filepath = os.path.join(self.config['models_dir'], filename)
        
        # ì €ì¥í•  ë°ì´í„° ì¤€ë¹„
        data = {
            'q_table': {state: q_values for state, q_values in self.q_table.items()},
            'episode': self.episode,
            'config': self.config,
            'statistics': {
                'episode_rewards': self.episode_rewards,
                'episode_steps': self.episode_steps,
            },
            'timestamp': time.time()
        }
        
        try:
            with open(filepath, 'w') as f:
                json.dump(data, f, indent=2)
            print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {filepath}")
            return filepath
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            return ""
    
    def load_model(self, filepath: str) -> bool:
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            with open(filepath, 'r') as f:
                data = json.load(f)
            
            # Q-table ë¡œë“œ
            self.q_table = data['q_table']
            
            # ì„¤ì • ì—…ë°ì´íŠ¸
            if 'config' in data:
                self.config.update(data['config'])
            
            # í†µê³„ ë¡œë“œ
            if 'statistics' in data:
                self.episode_rewards = data['statistics'].get('episode_rewards', [])
                self.episode_steps = data['statistics'].get('episode_steps', [])
            
            if 'episode' in data:
                self.episode = data['episode']
            
            print(f"ğŸ“‚ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {filepath}")
            print(f"  Q-table í¬ê¸°: {len(self.q_table)} ìƒíƒœ")
            print(f"  ì—í”¼ì†Œë“œ: {self.episode}")
            return True
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def run_trained_model(self, max_steps: int = 2000) -> None:
        """í›ˆë ¨ëœ ëª¨ë¸ë¡œ ì‹¤í–‰ (íƒí—˜ ì—†ì´)"""
        print("ğŸ® í›ˆë ¨ëœ ëª¨ë¸ë¡œ ë¼ì¸ íŠ¸ë ˆì´ì‹± ì‹œì‘!")
        print("Ctrl+Cë¡œ ì¤‘ë‹¨")
        
        # íƒí—˜ë¥ ì„ 0ìœ¼ë¡œ ì„¤ì • (ìˆœìˆ˜ í™œìš©)
        original_exploration = self.config['exploration_rate']
        self.config['exploration_rate'] = 0.0
        
        try:
            step = 0
            while step < max_steps:
                # í˜„ì¬ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
                state = self.get_state()
                frame = self.capture_frame()
                
                if frame is not None:
                    line_x, line_detected = self.process_frame(frame)
                    
                    # ìµœì  ì•¡ì…˜ ì„ íƒ
                    action = self.choose_action(state)
                    action_name = self._get_action_name(action)
                    
                    # ì•¡ì…˜ ì‹¤í–‰
                    self.execute_action(action)
                    
                    # ìƒíƒœ ì¶œë ¥
                    if step % 20 == 0:
                        print(f"ìŠ¤í… {step}: ë¼ì¸ìœ„ì¹˜={line_x}, ê²€ì¶œ={line_detected}, ì•¡ì…˜={action_name}")
                    
                    step += 1
                    time.sleep(0.1)
                else:
                    print("ì¹´ë©”ë¼ ì˜¤ë¥˜!")
                    break
                
        except KeyboardInterrupt:
            print("\nâ¹ï¸ ì‹¤í–‰ ì¤‘ë‹¨")
        
        finally:
            self.motor.stop_motors()
            self.config['exploration_rate'] = original_exploration
            print("ğŸ ì‹¤í–‰ ì™„ë£Œ")
    
    def cleanup(self) -> None:
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.motor.stop_motors()
        self.motor.cleanup()
        if self.camera.isOpened():
            self.camera.release()
        print("ğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ¤– Q-Learning ë¼ì¸ íŠ¸ë ˆì´ì‹± ì‹œìŠ¤í…œ")
    print("=" * 50)
    
    # Q-Learning ì—ì´ì „íŠ¸ ìƒì„±
    agent = LineTrackingQLearning()
    
    try:
        while True:
            print("\në©”ë‰´:")
            print("1. ìƒˆë¡œìš´ í›ˆë ¨ ì‹œì‘")
            print("2. ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ í›„ í›ˆë ¨ ê³„ì†")
            print("3. í›ˆë ¨ëœ ëª¨ë¸ë¡œ ë¼ì¸ íŠ¸ë ˆì´ì‹±")
            print("4. ì¢…ë£Œ")
            
            choice = input("ì„ íƒí•˜ì„¸ìš” (1-4): ").strip()
            
            if choice == '1':
                episodes = int(input("í›ˆë ¨í•  ì—í”¼ì†Œë“œ ìˆ˜ (ê¸°ë³¸ 200): ") or "200")
                agent.start_training(episodes)
                
            elif choice == '2':
                model_path = input("ëª¨ë¸ íŒŒì¼ ê²½ë¡œ: ").strip()
                if agent.load_model(model_path):
                    episodes = int(input("ì¶”ê°€ í›ˆë ¨í•  ì—í”¼ì†Œë“œ ìˆ˜ (ê¸°ë³¸ 100): ") or "100")
                    agent.start_training(episodes)
                
            elif choice == '3':
                model_path = input("ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸: Q_LineTracking/models/final_line_model.json): ").strip()
                if not model_path:
                    model_path = "Q_LineTracking/models/final_line_model.json"
                
                if agent.load_model(model_path):
                    max_steps = int(input("ìµœëŒ€ ì‹¤í–‰ ìŠ¤í… ìˆ˜ (ê¸°ë³¸ 2000): ") or "2000")
                    agent.run_trained_model(max_steps)
                
            elif choice == '4':
                break
                
            else:
                print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
    
    except KeyboardInterrupt:
        print("\ní”„ë¡œê·¸ë¨ ì¤‘ë‹¨")
    
    finally:
        agent.cleanup()


if __name__ == "__main__":
    main() 