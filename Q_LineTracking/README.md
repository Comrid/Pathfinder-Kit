# ğŸ¤– Q-Learning ë¼ì¸ íŠ¸ë ˆì´ì‹± ì‹œìŠ¤í…œ

íŒ¨ìŠ¤íŒŒì¸ë” í‚¤íŠ¸ë¥¼ ìœ„í•œ ê°•í™”í•™ìŠµ ê¸°ë°˜ ë¼ì¸ íŠ¸ë ˆì´ì‹± ì‹œìŠ¤í…œì…ë‹ˆë‹¤. Q-Learning ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ì¹´ë©”ë¼ë¡œ ê²€ì€ìƒ‰ ë¼ì¸ì„ ë”°ë¼ê°€ëŠ” ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨
- [ì‹œìŠ¤í…œ ê°œìš”](#ì‹œìŠ¤í…œ-ê°œìš”)
- [ì„¤ì¹˜ ë° ì„¤ì •](#ì„¤ì¹˜-ë°-ì„¤ì •)
- [ì‚¬ìš©ë²•](#ì‚¬ìš©ë²•)
- [ì›¹ ì¸í„°í˜ì´ìŠ¤](#ì›¹-ì¸í„°í˜ì´ìŠ¤)
- [Q-Learning íŒŒë¼ë¯¸í„°](#q-learning-íŒŒë¼ë¯¸í„°)
- [í•™ìŠµ ê³¼ì •](#í•™ìŠµ-ê³¼ì •)
- [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)

## ğŸ¯ ì‹œìŠ¤í…œ ê°œìš”

### ê¸°ì¡´ PID vs Q-Learning ë¹„êµ

| íŠ¹ì§• | PID ì œì–´ | Q-Learning |
|------|----------|------------|
| **í•™ìŠµ ë°©ì‹** | ìˆ˜ë™ íŠœë‹ | ìë™ í•™ìŠµ |
| **ì ì‘ì„±** | ê³ ì •ëœ íŒŒë¼ë¯¸í„° | í™˜ê²½ì— ì ì‘ |
| **ë³µì¡í•œ ìƒí™©** | ì œí•œì  ëŒ€ì‘ | ìœ ì—°í•œ ëŒ€ì‘ |
| **ì„¤ì • ë‚œì´ë„** | ë†’ìŒ (ì „ë¬¸ ì§€ì‹ í•„ìš”) | ë‚®ìŒ (ìë™ ìµœì í™”) |
| **ì„±ëŠ¥** | ì¦‰ì‹œ ìµœì  | ì ì§„ì  í–¥ìƒ |

### Q-Learning ë¼ì¸ íŠ¸ë ˆì´ì‹±ì˜ ì¥ì 

1. **ìë™ ìµœì í™”**: í™˜ê²½ì— ë§ê²Œ ìŠ¤ìŠ¤ë¡œ í•™ìŠµ
2. **ë³µì¡í•œ ìƒí™© ëŒ€ì‘**: ê³¡ì„ , êµì°¨ì , ëŠì–´ì§„ ë¼ì¸ ë“±
3. **ë…¸ì´ì¦ˆ ë‚´ì„±**: ì¡°ëª… ë³€í™”, ê·¸ë¦¼ìì— ê°•í•¨
4. **í™•ì¥ì„±**: ë‹¤ì–‘í•œ ì„¼ì„œ ì •ë³´ í†µí•© ê°€ëŠ¥

## ğŸ”§ ì„¤ì¹˜ ë° ì„¤ì •

### í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬
```bash
pip install opencv-python numpy flask
```

### í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­
- Raspberry Pi Zero 2 W
- 160ë„ ê´‘ê° ì¹´ë©”ë¼ ëª¨ë“ˆ
- L298N ëª¨í„° ë“œë¼ì´ë²„
- DC ê¸°ì–´ ëª¨í„° 2ê°œ

### íŒŒì¼ êµ¬ì¡°
```
Q_LineTracking/
â”œâ”€â”€ qlearning_line_tracker.py    # ë©”ì¸ Q-Learning êµ¬í˜„
â”œâ”€â”€ qlearning_line_web.py        # ì›¹ ì¸í„°í˜ì´ìŠ¤
â”œâ”€â”€ models/                      # ì €ì¥ëœ ëª¨ë¸ë“¤
â””â”€â”€ README.md                    # ì´ íŒŒì¼
```

## ğŸš€ ì‚¬ìš©ë²•

### 1. í„°ë¯¸ë„ ëª¨ë“œ

```bash
cd Q_LineTracking
python qlearning_line_tracker.py
```

**ë©”ë‰´ ì˜µì…˜:**
1. **ìƒˆë¡œìš´ í›ˆë ¨ ì‹œì‘**: ì²˜ìŒë¶€í„° í•™ìŠµ
2. **ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ í›„ í›ˆë ¨ ê³„ì†**: ì €ì¥ëœ ëª¨ë¸ì—ì„œ ì´ì–´ì„œ í•™ìŠµ
3. **í›ˆë ¨ëœ ëª¨ë¸ë¡œ ë¼ì¸ íŠ¸ë ˆì´ì‹±**: í•™ìŠµ ì™„ë£Œëœ ëª¨ë¸ë¡œ ì‹¤í–‰
4. **ì¢…ë£Œ**: í”„ë¡œê·¸ë¨ ì¢…ë£Œ

### 2. ì›¹ ì¸í„°í˜ì´ìŠ¤ ëª¨ë“œ (ì¶”ì²œ)

```bash
cd Q_LineTracking
python qlearning_line_web.py
```

ë¸Œë¼ìš°ì €ì—ì„œ `http://ë¼ì¦ˆë² ë¦¬íŒŒì´IP:5001` ì ‘ì†

## ğŸŒ ì›¹ ì¸í„°í˜ì´ìŠ¤

### ì£¼ìš” ê¸°ëŠ¥

#### ğŸ® í›ˆë ¨ ì œì–´
- **í›ˆë ¨ ì‹œì‘/ì¤‘ë‹¨**: ì‹¤ì‹œê°„ í•™ìŠµ ì œì–´
- **ì—í”¼ì†Œë“œ ìˆ˜ ì„¤ì •**: í•™ìŠµí•  ì—í”¼ì†Œë“œ ê°œìˆ˜ (ê¸°ë³¸: 200)
- **ìµœëŒ€ ìŠ¤í… ì„¤ì •**: ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ í–‰ë™ ìˆ˜ (ê¸°ë³¸: 1000)
- **í›ˆë ¨ëœ ëª¨ë¸ ì‹¤í–‰**: í•™ìŠµ ì™„ë£Œ í›„ ì‹¤ì œ ë¼ì¸ íŠ¸ë ˆì´ì‹±

#### ğŸ“¹ ì‹¤ì‹œê°„ ì¹´ë©”ë¼ í”¼ë“œ
- **ë¼ì´ë¸Œ ì˜ìƒ**: ì‹¤ì‹œê°„ ì¹´ë©”ë¼ í™”ë©´
- **ë¼ì¸ ê²€ì¶œ ì‹œê°í™”**: ê²€ì¶œëœ ë¼ì¸ê³¼ ROI í‘œì‹œ
- **ì¤‘ì•™ì„  ê°€ì´ë“œ**: ëª©í‘œ ìœ„ì¹˜ í‘œì‹œ
- **í”„ë ˆì„ ìº¡ì²˜**: íŠ¹ì • ìˆœê°„ ì €ì¥

#### ğŸ“Š ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
- **í˜„ì¬ ì—í”¼ì†Œë“œ/ìŠ¤í…**: í•™ìŠµ ì§„í–‰ ìƒí™©
- **í˜„ì¬ ë³´ìƒ**: ì‹¤ì‹œê°„ ë³´ìƒ ê°’
- **ë¼ì¸ ìœ„ì¹˜**: ê²€ì¶œëœ ë¼ì¸ì˜ X ì¢Œí‘œ
- **ë¼ì¸ ê²€ì¶œ ìƒíƒœ**: âœ…/âŒ í‘œì‹œ
- **íƒí—˜ë¥ **: í˜„ì¬ Îµ(epsilon) ê°’
- **Q-Table í¬ê¸°**: í•™ìŠµëœ ìƒíƒœ ìˆ˜

#### âš™ï¸ ì‹¤ì‹œê°„ íŒŒë¼ë¯¸í„° ì¡°ì •
- **í•™ìŠµë¥  (Î±)**: 0.01 ~ 0.5 (ê¸°ë³¸: 0.15)
- **í• ì¸ ì¸ìˆ˜ (Î³)**: 0.1 ~ 1.0 (ê¸°ë³¸: 0.9)
- **íƒí—˜ë¥  ê°ì†Œ**: 0.99 ~ 0.999 (ê¸°ë³¸: 0.998)
- **ë¼ì¸ ì„ê³„ê°’**: 20 ~ 100 (ê¸°ë³¸: 50)
- **ê¸°ë³¸ ì†ë„**: 20 ~ 60 (ê¸°ë³¸: 35)

#### ğŸ“ˆ í•™ìŠµ ì§„í–‰ ì°¨íŠ¸
- **ì—í”¼ì†Œë“œë³„ ë³´ìƒ**: ì‹¤ì‹œê°„ í•™ìŠµ ì„±ê³¼
- **í‰ê·  ë³´ìƒ**: ìµœê·¼ 10 ì—í”¼ì†Œë“œ í‰ê· 
- **í•™ìŠµ ê³¡ì„ **: ì‹œê°„ì— ë”°ë¥¸ ì„±ëŠ¥ í–¥ìƒ

## âš™ï¸ Q-Learning íŒŒë¼ë¯¸í„°

### í•µì‹¬ íŒŒë¼ë¯¸í„°

#### í•™ìŠµë¥  (Learning Rate, Î±)
```python
learning_rate = 0.15  # ê¸°ë³¸ê°’
```
- **ë†’ìŒ (0.3-0.5)**: ë¹ ë¥¸ í•™ìŠµ, ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŒ
- **ì¤‘ê°„ (0.1-0.2)**: ê· í˜•ì¡íŒ í•™ìŠµ (ê¶Œì¥)
- **ë‚®ìŒ (0.01-0.1)**: ì•ˆì •ì ì´ì§€ë§Œ ëŠë¦° í•™ìŠµ

#### í• ì¸ ì¸ìˆ˜ (Discount Factor, Î³)
```python
discount_factor = 0.9  # ê¸°ë³¸ê°’
```
- **ë†’ìŒ (0.9-0.99)**: ì¥ê¸°ì  ë³´ìƒ ì¤‘ì‹œ (ë¼ì¸ íŠ¸ë ˆì´ì‹±ì— ì í•©)
- **ë‚®ìŒ (0.1-0.5)**: ì¦‰ì‹œ ë³´ìƒ ì¤‘ì‹œ

#### íƒí—˜ë¥  (Exploration Rate, Îµ)
```python
exploration_rate = 1.0      # ì´ˆê¸°ê°’ (100% íƒí—˜)
exploration_min = 0.05      # ìµœì†Œê°’ (5% íƒí—˜)
exploration_decay = 0.998   # ê°ì†Œìœ¨
```

### ë¼ì¸ íŠ¸ë ˆì´ì‹± ì„¤ì •

#### ìƒíƒœ ê³µê°„
```python
line_bins = 7              # ë¼ì¸ ìœ„ì¹˜ êµ¬ê°„ ìˆ˜
```
í™”ë©´ì„ 7ê°œ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë¼ì¸ ìœ„ì¹˜ë¥¼ ì´ì‚°í™”:
- êµ¬ê°„ 0: ì™¼ìª½ ë (0-45px)
- êµ¬ê°„ 1: ì™¼ìª½ (46-91px)
- êµ¬ê°„ 2: ì™¼ìª½ ì¤‘ì•™ (92-137px)
- êµ¬ê°„ 3: ì¤‘ì•™ (138-183px) â† ëª©í‘œ
- êµ¬ê°„ 4: ì˜¤ë¥¸ìª½ ì¤‘ì•™ (184-229px)
- êµ¬ê°„ 5: ì˜¤ë¥¸ìª½ (230-275px)
- êµ¬ê°„ 6: ì˜¤ë¥¸ìª½ ë (276-320px)

#### ì•¡ì…˜ ê³µê°„
```python
actions = [
    (35, 35),    # 0: ì§ì§„
    (20, 35),    # 1: ì•½ê°„ ì¢ŒíšŒì „
    (35, 20),    # 2: ì•½ê°„ ìš°íšŒì „
    (10, 50),    # 3: ê°•í•œ ì¢ŒíšŒì „
    (50, 10),    # 4: ê°•í•œ ìš°íšŒì „
    (0, 35),     # 5: ì œìë¦¬ ì¢ŒíšŒì „
    (35, 0),     # 6: ì œìë¦¬ ìš°íšŒì „
    (15, 15),    # 7: ëŠë¦° ì§ì§„
]
```

#### ë³´ìƒ ì‹œìŠ¤í…œ
```python
rewards = {
    'center_bonus': 15,      # ì¤‘ì•™ì— ìˆì„ ë•Œ (ìµœê³  ë³´ìƒ)
    'on_line': 10,           # ë¼ì¸ ìœ„ì— ìˆì„ ë•Œ
    'near_line': 5,          # ë¼ì¸ ê·¼ì²˜ì— ìˆì„ ë•Œ
    'off_line': -5,          # ë¼ì¸ì—ì„œ ë²—ì–´ë‚¬ì„ ë•Œ
    'line_lost': -20,        # ë¼ì¸ì„ ì™„ì „íˆ ë†“ì³¤ì„ ë•Œ (í˜ë„í‹°)
    'step_penalty': -0.5,    # ìŠ¤í… í˜ë„í‹° (íš¨ìœ¨ì„± ìœ ë„)
}
```

## ğŸ“Š í•™ìŠµ ê³¼ì •

### í•™ìŠµ ë‹¨ê³„ë³„ íŠ¹ì§•

#### 1ë‹¨ê³„: ì´ˆê¸° íƒí—˜ (0-50 ì—í”¼ì†Œë“œ)
- **íŠ¹ì§•**: ëœë¤í•œ ì›€ì§ì„, ë†’ì€ Îµ ê°’ (1.0 â†’ 0.6)
- **ëª©ì **: ë‹¤ì–‘í•œ ìƒí™© ê²½í—˜, ê¸°ë³¸ ìƒíƒœ-ì•¡ì…˜ ë§¤í•‘
- **ì˜ˆìƒ ë³´ìƒ**: -50 ~ -10 (ë§ì€ ì‹¤ìˆ˜)

#### 2ë‹¨ê³„: ê¸°ë³¸ í•™ìŠµ (50-100 ì—í”¼ì†Œë“œ)
- **íŠ¹ì§•**: ì ì§„ì  ê°œì„ , ì¤‘ê°„ Îµ ê°’ (0.6 â†’ 0.3)
- **ëª©ì **: ê¸°ë³¸ì ì¸ ë¼ì¸ ì¶”ì  ëŠ¥ë ¥ ìŠµë“
- **ì˜ˆìƒ ë³´ìƒ**: -10 ~ 20 (ê°€ë” ì„±ê³µ)

#### 3ë‹¨ê³„: ì•ˆì •í™” (100-150 ì—í”¼ì†Œë“œ)
- **íŠ¹ì§•**: ì¼ê´€ëœ ì„±ëŠ¥, ë‚®ì€ Îµ ê°’ (0.3 â†’ 0.15)
- **ëª©ì **: ì•ˆì •ì ì¸ ë¼ì¸ íŠ¸ë ˆì´ì‹±
- **ì˜ˆìƒ ë³´ìƒ**: 20 ~ 60 (ëŒ€ë¶€ë¶„ ì„±ê³µ)

#### 4ë‹¨ê³„: ìµœì í™” (150+ ì—í”¼ì†Œë“œ)
- **íŠ¹ì§•**: ìµœì  ê²½ë¡œ íƒìƒ‰, ìµœì†Œ Îµ ê°’ (0.15 â†’ 0.05)
- **ëª©ì **: ë¶€ë“œëŸ½ê³  íš¨ìœ¨ì ì¸ ì›€ì§ì„
- **ì˜ˆìƒ ë³´ìƒ**: 60 ~ 100+ (ìµœì  ì„±ëŠ¥)

### ì„±ëŠ¥ ì§€í‘œ

#### Q-Table í¬ê¸° ë³€í™”
- **ì´ˆê¸°**: 5-10 ìƒíƒœ
- **ì¤‘ê¸°**: 15-25 ìƒíƒœ
- **í›„ê¸°**: 20-30 ìƒíƒœ (ìˆ˜ë ´)

#### ì—í”¼ì†Œë“œ ê¸¸ì´
- **ì´ˆê¸°**: 50-200 ìŠ¤í… (ë¹¨ë¦¬ ì‹¤íŒ¨)
- **ì¤‘ê¸°**: 200-500 ìŠ¤í… (ì ì§„ì  ê°œì„ )
- **í›„ê¸°**: 800-1000 ìŠ¤í… (ìµœëŒ€ ê¸¸ì´ ë‹¬ì„±)

## ğŸ”§ ê³ ê¸‰ ì„¤ì •

### í™˜ê²½ë³„ ìµœì í™”

#### ì‹¤ë‚´ í˜•ê´‘ë“± í™˜ê²½
```python
config = {
    'line_threshold': 60,        # ë†’ì€ ì„ê³„ê°’
    'roi_height': 100,           # ë„“ì€ ROI
    'min_line_area': 150,        # í° ìµœì†Œ ë©´ì 
}
```

#### ìì—°ê´‘ í™˜ê²½
```python
config = {
    'line_threshold': 40,        # ë‚®ì€ ì„ê³„ê°’
    'roi_height': 80,            # ê¸°ë³¸ ROI
    'min_line_area': 100,        # ê¸°ë³¸ ìµœì†Œ ë©´ì 
}
```

#### ì–´ë‘ìš´ í™˜ê²½
```python
config = {
    'line_threshold': 30,        # ë§¤ìš° ë‚®ì€ ì„ê³„ê°’
    'roi_height': 120,           # ë„“ì€ ROI
    'min_line_area': 80,         # ì‘ì€ ìµœì†Œ ë©´ì 
}
```

### ìƒíƒœ ê³µê°„ í™•ì¥

#### ê¸°ë³¸ ìƒíƒœ (í˜„ì¬)
```python
state = f"line{line_bin}"
```

#### í™•ì¥ ìƒíƒœ ì˜ˆì‹œ
```python
# ë¼ì¸ ìœ„ì¹˜ + ë°©í–¥ ì •ë³´
state = f"line{line_bin}_dir{direction_bin}"

# ë¼ì¸ ìœ„ì¹˜ + ì†ë„ ì •ë³´
state = f"line{line_bin}_speed{speed_bin}"

# ë¼ì¸ ìœ„ì¹˜ + ì´ì „ ì•¡ì…˜
state = f"line{line_bin}_prev{prev_action}"
```

### ë³´ìƒ í•¨ìˆ˜ ì»¤ìŠ¤í„°ë§ˆì´ì§•

#### ë¶€ë“œëŸ¬ìš´ ì›€ì§ì„ ì¥ë ¤
```python
def smooth_reward(self, line_x, line_detected, action_idx, prev_action):
    base_reward = self.get_reward(line_x, line_detected, action_idx)
    
    # ê¸‰ê²©í•œ ë°©í–¥ ì „í™˜ í˜ë„í‹°
    if abs(action_idx - prev_action) > 2:
        base_reward -= 5
    
    # ì§ì§„ ë³´ë„ˆìŠ¤ (ë¼ì¸ì´ ì¤‘ì•™ì— ìˆì„ ë•Œ)
    if action_idx == 0 and abs(line_x - 160) < 30:
        base_reward += 3
    
    return base_reward
```

#### ì†ë„ ê¸°ë°˜ ë³´ìƒ
```python
def speed_reward(self, line_x, line_detected, action_idx):
    base_reward = self.get_reward(line_x, line_detected, action_idx)
    
    # ë¹ ë¥¸ ì†ë„ ë³´ë„ˆìŠ¤ (ì§ì„  êµ¬ê°„)
    if action_idx in [0, 7] and abs(line_x - 160) < 20:
        base_reward += 2
    
    # ëŠë¦° ì†ë„ ë³´ë„ˆìŠ¤ (ê³¡ì„  êµ¬ê°„)
    elif action_idx == 7 and abs(line_x - 160) > 50:
        base_reward += 1
    
    return base_reward
```

## ğŸ› ë¬¸ì œ í•´ê²°

### í•™ìŠµ ê´€ë ¨ ë¬¸ì œ

#### í•™ìŠµì´ ì•ˆ ë˜ëŠ” ê²½ìš°
**ì¦ìƒ**: ë³´ìƒì´ ê³„ì† ìŒìˆ˜, ì„±ëŠ¥ ê°œì„  ì—†ìŒ
**ì›ì¸**: 
- í•™ìŠµë¥ ì´ ë„ˆë¬´ ë‚®ìŒ
- íƒí—˜ë¥ ì´ ë„ˆë¬´ ë¹¨ë¦¬ ê°ì†Œ
- ë³´ìƒ ì„¤ê³„ ë¬¸ì œ

**í•´ê²°ì±…**:
```python
# í•™ìŠµë¥  ì¦ê°€
learning_rate = 0.2

# íƒí—˜ë¥  ê°ì†Œ ì†ë„ ì¡°ì •
exploration_decay = 0.999
exploration_min = 0.1

# ë³´ìƒ ì¬ì¡°ì •
rewards['center_bonus'] = 20
rewards['step_penalty'] = -0.1
```

#### ê³¼í•™ìŠµ (Overfitting)
**ì¦ìƒ**: íŠ¹ì • í™˜ê²½ì—ì„œë§Œ ë™ì‘, ì¼ë°˜í™” ë¶€ì¡±
**í•´ê²°ì±…**:
- ë‹¤ì–‘í•œ ì¡°ëª… ì¡°ê±´ì—ì„œ í›ˆë ¨
- íƒí—˜ë¥  ìµœì†Œê°’ ì¦ê°€ (0.1 ì´ìƒ)
- ì •ê·œí™” ê¸°ë²• ì ìš©

#### ë¶ˆì•ˆì •í•œ í•™ìŠµ
**ì¦ìƒ**: ë³´ìƒì´ í¬ê²Œ ìš”ë™ì¹¨
**í•´ê²°ì±…**:
```python
# í• ì¸ ì¸ìˆ˜ ì¦ê°€ (ì¥ê¸°ì  ê´€ì )
discount_factor = 0.95

# í•™ìŠµë¥  ê°ì†Œ
learning_rate = 0.1

# ë³´ìƒ í‰í™œí™”
def smoothed_reward(self, rewards_history):
    return np.mean(rewards_history[-5:])  # ìµœê·¼ 5ê°œ í‰ê· 
```

### í•˜ë“œì›¨ì–´ ê´€ë ¨ ë¬¸ì œ

#### ì¹´ë©”ë¼ ì¸ì‹ ë¶ˆëŸ‰
**ì¦ìƒ**: ë¼ì¸ ê²€ì¶œ ì‹¤íŒ¨, "No Line" ì§€ì†
**í•´ê²°ì±…**:
```python
# ì„ê³„ê°’ ì¡°ì •
line_threshold = 30  # ë” ë‚®ê²Œ

# ROI í™•ì¥
roi_height = 120
roi_y_offset = 140

# ìµœì†Œ ë©´ì  ê°ì†Œ
min_line_area = 50
```

#### ëª¨í„° ë°˜ì‘ ì§€ì—°
**ì¦ìƒ**: ì•¡ì…˜ê³¼ ì‹¤ì œ ì›€ì§ì„ ë¶ˆì¼ì¹˜
**í•´ê²°ì±…**:
```python
# ì•¡ì…˜ ì‹¤í–‰ í›„ ëŒ€ê¸° ì‹œê°„ ì¦ê°€
time.sleep(0.15)  # 0.1ì—ì„œ 0.15ë¡œ

# ì†ë„ ì¡°ì •
base_speed = 30  # ë” ëŠë¦¬ê²Œ
turn_speed = 45
```

#### ì „ì› ë¶€ì¡±
**ì¦ìƒ**: ê°‘ì‘ìŠ¤ëŸ° ì •ì§€, ì„±ëŠ¥ ì €í•˜
**í•´ê²°ì±…**:
- ë°°í„°ë¦¬ ì¶©ì „ ìƒíƒœ í™•ì¸
- ì „ì•• ê°•í•˜ ëª¨ë“ˆ ì¶œë ¥ í™•ì¸
- ëª¨í„° ì†ë„ ê°ì†Œë¡œ ì „ë ¥ ì†Œëª¨ ì¤„ì´ê¸°

### ì„±ëŠ¥ ìµœì í™”

#### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
```python
# Q-table í¬ê¸° ì œí•œ
MAX_STATES = 100

def cleanup_qtable(self):
    if len(self.q_table) > MAX_STATES:
        # ì‚¬ìš© ë¹ˆë„ê°€ ë‚®ì€ ìƒíƒœ ì œê±°
        sorted_states = sorted(self.q_table.items(), 
                              key=lambda x: max(x[1]))
        for state, _ in sorted_states[:20]:
            del self.q_table[state]
```

#### í•™ìŠµ ì†ë„ í–¥ìƒ
```python
# ê²½í—˜ ì¬ìƒ (Experience Replay)
class ExperienceBuffer:
    def __init__(self, size=1000):
        self.buffer = []
        self.size = size
    
    def add(self, experience):
        if len(self.buffer) >= self.size:
            self.buffer.pop(0)
        self.buffer.append(experience)
    
    def sample(self, batch_size=32):
        return random.sample(self.buffer, 
                           min(batch_size, len(self.buffer)))
```

## ğŸ“š ì¶”ê°€ í•™ìŠµ ìë£Œ

### ê°•í™”í•™ìŠµ ì´ë¡ 
- [Q-Learning ê¸°ì´ˆ](https://en.wikipedia.org/wiki/Q-learning)
- [Temporal Difference Learning](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)

### ì»´í“¨í„° ë¹„ì „
- [OpenCV ë¼ì¸ ê²€ì¶œ](https://docs.opencv.org/4.x/d9/db0/tutorial_hough_lines.html)
- [ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ê¸°ë²•](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_table_of_contents_imgproc/py_table_of_contents_imgproc.html)

### ì‹¤ìŠµ ì˜ˆì œ
- [Q-Learning ì‹œê°í™”](https://cs.stanford.edu/people/karpathy/reinforcejs/)
- [ë¼ì¸ íŠ¸ë ˆì´ì‹± ì•Œê³ ë¦¬ì¦˜](https://github.com/topics/line-following-robot)

## ğŸ¯ í”„ë¡œì íŠ¸ í™•ì¥ ì•„ì´ë””ì–´

### 1. ê³ ê¸‰ ìƒíƒœ í‘œí˜„
- **ë‹¤ì¤‘ í”„ë ˆì„**: ì—°ì†ëœ í”„ë ˆì„ìœ¼ë¡œ ì›€ì§ì„ ì •ë³´ ì¶”ê°€
- **ë¼ì¸ ê³¡ë¥ **: ë¼ì¸ì˜ êµ½ì€ ì •ë„ ì¸¡ì •
- **êµì°¨ì  ê°ì§€**: Tì, ì‹­ì êµì°¨ì  ì¸ì‹

### 2. ë‹¤ì¤‘ ëª©í‘œ í•™ìŠµ
- **ì†ë„ ìµœì í™”**: ë¹ ë¥´ë©´ì„œë„ ì•ˆì •ì ì¸ ì£¼í–‰
- **ì—ë„ˆì§€ íš¨ìœ¨**: ìµœì†Œ ì „ë ¥ìœ¼ë¡œ ëª©í‘œ ë‹¬ì„±
- **ë¶€ë“œëŸ¬ìš´ ì›€ì§ì„**: ê¸‰ê²©í•œ ë°©í–¥ ì „í™˜ ìµœì†Œí™”

### 3. í™˜ê²½ ì ì‘
- **ì¡°ëª… ë³€í™”**: ìë™ ì„ê³„ê°’ ì¡°ì •
- **ë‹¤ì–‘í•œ ë¼ì¸**: ìƒ‰ìƒ, ë‘ê»˜ ë³€í™” ëŒ€ì‘
- **ì¥ì• ë¬¼ íšŒí”¼**: ë¼ì¸ íŠ¸ë ˆì´ì‹± + ì¥ì• ë¬¼ ê°ì§€

### 4. ê³ ê¸‰ ì•Œê³ ë¦¬ì¦˜
- **Deep Q-Network (DQN)**: ì‹ ê²½ë§ ê¸°ë°˜ Q-Learning
- **Double DQN**: ê³¼ì¶”ì • ë¬¸ì œ í•´ê²°
- **Dueling DQN**: ìƒíƒœ ê°€ì¹˜ì™€ ì•¡ì…˜ ì´ì  ë¶„ë¦¬

---

ğŸ‰ **Q-Learningìœ¼ë¡œ ë˜‘ë˜‘í•œ ë¼ì¸ íŠ¸ë ˆì´ì‹± ë¡œë´‡ì„ ë§Œë“¤ì–´ë³´ì„¸ìš”!** 