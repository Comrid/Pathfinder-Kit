#!/usr/bin/env python3
"""
ğŸ§ª Q-Learning LineTracing System Test Script
íŒ¨ìŠ¤íŒŒì¸ë” í‚¤íŠ¸ Q-Learning ì‹œìŠ¤í…œ ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” í•˜ë“œì›¨ì–´ ì—†ì´ë„ Q-Learning ì•Œê³ ë¦¬ì¦˜ì˜ 
ê¸°ë³¸ ë™ì‘ì„ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

import numpy as np
import json
import time
from datetime import datetime

# ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ í´ë˜ìŠ¤ ì„í¬íŠ¸
try:
    from app import QLearningConfig, QLearningAgent
    print("âœ… ë©”ì¸ ëª¨ë“ˆ ì„í¬íŠ¸ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ ë©”ì¸ ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    exit(1)

def test_qlearning_config():
    """Q-Learning ì„¤ì • í…ŒìŠ¤íŠ¸"""
    print("\nğŸ”§ Q-Learning ì„¤ì • í…ŒìŠ¤íŠ¸")
    print("-" * 40)
    
    config = QLearningConfig()
    
    # ê¸°ë³¸ íŒŒë¼ë¯¸í„° í™•ì¸
    assert config.learning_rate == 0.1, "í•™ìŠµë¥  ì˜¤ë¥˜"
    assert config.discount_factor == 0.95, "í• ì¸ì¸ìˆ˜ ì˜¤ë¥˜"
    assert config.epsilon == 0.3, "íƒí—˜ë¥  ì˜¤ë¥˜"
    assert config.num_states == 7, "ìƒíƒœ ìˆ˜ ì˜¤ë¥˜"
    assert config.num_actions == 5, "í–‰ë™ ìˆ˜ ì˜¤ë¥˜"
    
    print(f"âœ… í•™ìŠµë¥ : {config.learning_rate}")
    print(f"âœ… í• ì¸ì¸ìˆ˜: {config.discount_factor}")
    print(f"âœ… íƒí—˜ë¥ : {config.epsilon}")
    print(f"âœ… ìƒíƒœ ìˆ˜: {config.num_states}")
    print(f"âœ… í–‰ë™ ìˆ˜: {config.num_actions}")
    print(f"âœ… ê¸°ë³¸ ì†ë„: {config.base_speed}")
    print(f"âœ… íšŒì „ ì†ë„: {config.turn_speed}")

def test_qlearning_agent():
    """Q-Learning ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ¤– Q-Learning ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸")
    print("-" * 40)
    
    config = QLearningConfig()
    agent = QLearningAgent(config)
    
    # Q-í…Œì´ë¸” ì´ˆê¸°í™” í™•ì¸
    assert agent.q_table.shape == (7, 5), "Q-í…Œì´ë¸” í¬ê¸° ì˜¤ë¥˜"
    assert np.all(agent.q_table == 0), "Q-í…Œì´ë¸” ì´ˆê¸°í™” ì˜¤ë¥˜"
    
    print(f"âœ… Q-í…Œì´ë¸” í¬ê¸°: {agent.q_table.shape}")
    print(f"âœ… Q-í…Œì´ë¸” ì´ˆê¸°ê°’: ëª¨ë‘ 0")
    print(f"âœ… ì´ˆê¸° ì—í”¼ì†Œë“œ: {agent.episode}")
    print(f"âœ… ì´ˆê¸° ì´ ìŠ¤í…: {agent.total_steps}")

def test_state_conversion():
    """ìƒíƒœ ë³€í™˜ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ¯ ìƒíƒœ ë³€í™˜ í…ŒìŠ¤íŠ¸")
    print("-" * 40)
    
    config = QLearningConfig()
    agent = QLearningAgent(config)
    
    frame_width = 640
    test_cases = [
        (None, 3),      # ë¼ì¸ ì—†ìŒ â†’ ì¤‘ì•™ ìƒíƒœ
        (0, 0),         # ì™¼ìª½ ë
        (91, 1),        # ì™¼ìª½
        (182, 2),       # ì•½ê°„ ì™¼ìª½
        (320, 3),       # ì¤‘ì•™
        (457, 4),       # ì•½ê°„ ì˜¤ë¥¸ìª½
        (548, 5),       # ì˜¤ë¥¸ìª½
        (639, 6),       # ì˜¤ë¥¸ìª½ ë
    ]
    
    for line_x, expected_state in test_cases:
        state = agent.get_state_from_line_position(line_x, frame_width)
        assert state == expected_state, f"ìƒíƒœ ë³€í™˜ ì˜¤ë¥˜: {line_x} â†’ {state} (ì˜ˆìƒ: {expected_state})"
        print(f"âœ… ë¼ì¸ ìœ„ì¹˜ {line_x} â†’ ìƒíƒœ {state}")

def test_action_selection():
    """í–‰ë™ ì„ íƒ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ® í–‰ë™ ì„ íƒ í…ŒìŠ¤íŠ¸")
    print("-" * 40)
    
    config = QLearningConfig()
    agent = QLearningAgent(config)
    
    # íƒí—˜ ëª¨ë“œ í…ŒìŠ¤íŠ¸ (ëœë¤ í–‰ë™)
    actions_exploration = []
    for _ in range(100):
        action = agent.choose_action(3, training=True)  # ì¤‘ì•™ ìƒíƒœ
        actions_exploration.append(action)
        assert 0 <= action <= 4, f"ì˜ëª»ëœ í–‰ë™: {action}"
    
    unique_actions = len(set(actions_exploration))
    print(f"âœ… íƒí—˜ ëª¨ë“œ: {unique_actions}ê°œ ì„œë¡œ ë‹¤ë¥¸ í–‰ë™ ì„ íƒ")
    
    # Qê°’ ì„¤ì • í›„ í™œìš© ëª¨ë“œ í…ŒìŠ¤íŠ¸
    agent.q_table[3, 2] = 10.0  # ì¤‘ì•™ ìƒíƒœì—ì„œ ì§ì§„ í–‰ë™ì— ë†’ì€ Qê°’
    
    actions_exploitation = []
    for _ in range(10):
        action = agent.choose_action(3, training=False)  # í™œìš© ëª¨ë“œ
        actions_exploitation.append(action)
    
    # ëª¨ë“  í–‰ë™ì´ ì§ì§„(2)ì´ì–´ì•¼ í•¨
    assert all(action == 2 for action in actions_exploitation), "í™œìš© ëª¨ë“œ ì˜¤ë¥˜"
    print(f"âœ… í™œìš© ëª¨ë“œ: ìµœì  í–‰ë™(ì§ì§„) ì¼ê´€ì„± ìˆê²Œ ì„ íƒ")

def test_q_table_update():
    """Q-í…Œì´ë¸” ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ“Š Q-í…Œì´ë¸” ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸")
    print("-" * 40)
    
    config = QLearningConfig()
    agent = QLearningAgent(config)
    
    # ì´ˆê¸° Qê°’
    initial_q = agent.q_table[3, 2]  # ì¤‘ì•™ ìƒíƒœ, ì§ì§„ í–‰ë™
    print(f"âœ… ì´ˆê¸° Qê°’: {initial_q}")
    
    # Q-í…Œì´ë¸” ì—…ë°ì´íŠ¸
    state, action, reward, next_state = 3, 2, 15, 3
    agent.update_q_table(state, action, reward, next_state)
    
    # ì—…ë°ì´íŠ¸ëœ Qê°’
    updated_q = agent.q_table[3, 2]
    expected_q = initial_q + config.learning_rate * (reward + config.discount_factor * 0 - initial_q)
    
    assert abs(updated_q - expected_q) < 1e-6, f"Qê°’ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {updated_q} != {expected_q}"
    print(f"âœ… ì—…ë°ì´íŠ¸ëœ Qê°’: {updated_q:.3f}")
    print(f"âœ… ì˜ˆìƒ Qê°’: {expected_q:.3f}")

def test_reward_calculation():
    """ë³´ìƒ ê³„ì‚° í…ŒìŠ¤íŠ¸"""
    print("\nğŸ† ë³´ìƒ ê³„ì‚° í…ŒìŠ¤íŠ¸")
    print("-" * 40)
    
    config = QLearningConfig()
    agent = QLearningAgent(config)
    
    test_cases = [
        (3, True, config.reward_center),     # ì¤‘ì•™, ë¼ì¸ ê²€ì¶œë¨
        (2, True, config.reward_on_line),    # ì•½ê°„ ì™¼ìª½, ë¼ì¸ ê²€ì¶œë¨
        (4, True, config.reward_on_line),    # ì•½ê°„ ì˜¤ë¥¸ìª½, ë¼ì¸ ê²€ì¶œë¨
        (1, True, config.reward_on_line),    # ì™¼ìª½, ë¼ì¸ ê²€ì¶œë¨
        (5, True, config.reward_on_line),    # ì˜¤ë¥¸ìª½, ë¼ì¸ ê²€ì¶œë¨
        (0, True, config.reward_on_line - 3), # ì™¼ìª½ ë, ë¼ì¸ ê²€ì¶œë¨
        (6, True, config.reward_on_line - 3), # ì˜¤ë¥¸ìª½ ë, ë¼ì¸ ê²€ì¶œë¨
        (3, False, config.reward_off_line),  # ì¤‘ì•™, ë¼ì¸ ê²€ì¶œ ì•ˆë¨
    ]
    
    for state, line_detected, expected_reward in test_cases:
        reward = agent.calculate_reward(state, line_detected)
        assert reward == expected_reward, f"ë³´ìƒ ê³„ì‚° ì˜¤ë¥˜: ìƒíƒœ{state}, ê²€ì¶œ{line_detected} â†’ {reward} (ì˜ˆìƒ: {expected_reward})"
        print(f"âœ… ìƒíƒœ {state}, ë¼ì¸ ê²€ì¶œ {line_detected} â†’ ë³´ìƒ {reward}")

def test_model_save_load():
    """ëª¨ë¸ ì €ì¥/ë¡œë“œ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ’¾ ëª¨ë¸ ì €ì¥/ë¡œë“œ í…ŒìŠ¤íŠ¸")
    print("-" * 40)
    
    config = QLearningConfig()
    agent1 = QLearningAgent(config)
    
    # ì„ì˜ì˜ Qê°’ ì„¤ì •
    agent1.q_table[3, 2] = 10.5
    agent1.episode = 42
    agent1.total_steps = 1000
    agent1.successful_episodes = 20
    agent1.episode_rewards = [100, 150, 200]
    
    # ëª¨ë¸ ì €ì¥
    test_filename = "test_model.json"
    agent1.save_model(test_filename)
    print(f"âœ… ëª¨ë¸ ì €ì¥: {test_filename}")
    
    # ìƒˆ ì—ì´ì „íŠ¸ë¡œ ëª¨ë¸ ë¡œë“œ
    agent2 = QLearningAgent(config)
    success = agent2.load_model(test_filename)
    
    assert success, "ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨"
    assert agent2.q_table[3, 2] == 10.5, "Qê°’ ë¡œë“œ ì˜¤ë¥˜"
    assert agent2.episode == 42, "ì—í”¼ì†Œë“œ ë¡œë“œ ì˜¤ë¥˜"
    assert agent2.total_steps == 1000, "ì´ ìŠ¤í… ë¡œë“œ ì˜¤ë¥˜"
    assert agent2.successful_episodes == 20, "ì„±ê³µ ì—í”¼ì†Œë“œ ë¡œë“œ ì˜¤ë¥˜"
    assert agent2.episode_rewards == [100, 150, 200], "ì—í”¼ì†Œë“œ ë³´ìƒ ë¡œë“œ ì˜¤ë¥˜"
    
    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
    print(f"âœ… Qê°’ ë³µì›: {agent2.q_table[3, 2]}")
    print(f"âœ… ì—í”¼ì†Œë“œ ë³µì›: {agent2.episode}")
    print(f"âœ… ì´ ìŠ¤í… ë³µì›: {agent2.total_steps}")
    
    # í…ŒìŠ¤íŠ¸ íŒŒì¼ ì •ë¦¬
    import os
    try:
        os.remove(test_filename)
        print(f"âœ… í…ŒìŠ¤íŠ¸ íŒŒì¼ ì •ë¦¬: {test_filename}")
    except:
        pass

def test_episode_management():
    """ì—í”¼ì†Œë“œ ê´€ë¦¬ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ“ˆ ì—í”¼ì†Œë“œ ê´€ë¦¬ í…ŒìŠ¤íŠ¸")
    print("-" * 40)
    
    config = QLearningConfig()
    agent = QLearningAgent(config)
    
    # ì´ˆê¸° ìƒíƒœ
    initial_episode = agent.episode
    initial_epsilon = agent.config.epsilon
    
    # ë³´ìƒ ì„¤ì •
    agent.total_reward = 150
    
    # ì—í”¼ì†Œë“œ ì¢…ë£Œ
    agent.end_episode()
    
    # ê²€ì¦
    assert agent.episode == initial_episode + 1, "ì—í”¼ì†Œë“œ ì¦ê°€ ì˜¤ë¥˜"
    assert agent.total_reward == 0, "ë³´ìƒ ì´ˆê¸°í™” ì˜¤ë¥˜"
    assert len(agent.episode_rewards) == 1, "ì—í”¼ì†Œë“œ ë³´ìƒ ê¸°ë¡ ì˜¤ë¥˜"
    assert agent.episode_rewards[0] == 150, "ì—í”¼ì†Œë“œ ë³´ìƒ ê°’ ì˜¤ë¥˜"
    assert agent.config.epsilon < initial_epsilon, "íƒí—˜ë¥  ê°ì†Œ ì˜¤ë¥˜"
    assert agent.successful_episodes == 1, "ì„±ê³µ ì—í”¼ì†Œë“œ ì¹´ìš´íŠ¸ ì˜¤ë¥˜"
    
    print(f"âœ… ì—í”¼ì†Œë“œ ì¦ê°€: {initial_episode} â†’ {agent.episode}")
    print(f"âœ… ë³´ìƒ ì´ˆê¸°í™”: 150 â†’ {agent.total_reward}")
    print(f"âœ… íƒí—˜ë¥  ê°ì†Œ: {initial_epsilon:.3f} â†’ {agent.config.epsilon:.3f}")
    print(f"âœ… ì„±ê³µ ì—í”¼ì†Œë“œ: {agent.successful_episodes}")

def run_simulation():
    """ê°„ë‹¨í•œ í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜"""
    print("\nğŸ® í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜")
    print("-" * 40)
    
    config = QLearningConfig()
    agent = QLearningAgent(config)
    
    # ì‹œë®¬ë ˆì´ì…˜ íŒŒë¼ë¯¸í„°
    num_episodes = 10
    steps_per_episode = 20
    
    print(f"ğŸ“Š ì‹œë®¬ë ˆì´ì…˜ ì„¤ì •: {num_episodes} ì—í”¼ì†Œë“œ, ì—í”¼ì†Œë“œë‹¹ {steps_per_episode} ìŠ¤í…")
    
    for episode in range(num_episodes):
        agent.total_reward = 0
        
        for step in range(steps_per_episode):
            # ì„ì˜ì˜ ìƒíƒœ (ì¤‘ì•™ ê·¼ì²˜)
            state = np.random.choice([2, 3, 4])
            
            # í–‰ë™ ì„ íƒ
            action = agent.choose_action(state, training=True)
            
            # ë³´ìƒ ê³„ì‚° (ë¼ì¸ì´ ê²€ì¶œë˜ì—ˆë‹¤ê³  ê°€ì •)
            reward = agent.calculate_reward(state, True)
            agent.total_reward += reward
            
            # ë‹¤ìŒ ìƒíƒœ (ì„ì˜)
            next_state = np.random.choice([2, 3, 4])
            
            # Q-í…Œì´ë¸” ì—…ë°ì´íŠ¸
            if step > 0:  # ì²« ë²ˆì§¸ ìŠ¤í…ì´ ì•„ë‹Œ ê²½ìš°
                agent.update_q_table(prev_state, prev_action, reward, state)
            
            prev_state = state
            prev_action = action
        
        # ì—í”¼ì†Œë“œ ì¢…ë£Œ
        agent.end_episode()
        
        print(f"  ì—í”¼ì†Œë“œ {episode + 1}: ë³´ìƒ {agent.episode_rewards[-1]:.1f}, Îµ={agent.config.epsilon:.3f}")
    
    # ìµœì¢… Q-í…Œì´ë¸” ìƒíƒœ
    print(f"\nğŸ“Š ìµœì¢… Q-í…Œì´ë¸” (ì¤‘ì•™ ìƒíƒœ):")
    for action in range(config.num_actions):
        action_names = ['ì¢ŒíšŒì „', 'ì•½ê°„ì¢Œ', 'ì§ì§„', 'ì•½ê°„ìš°', 'ìš°íšŒì „']
        q_value = agent.q_table[3, action]
        print(f"  {action_names[action]}: {q_value:.3f}")

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸ§ª Q-Learning LineTracing System í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    start_time = time.time()
    
    try:
        # ê° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_qlearning_config()
        test_qlearning_agent()
        test_state_conversion()
        test_action_selection()
        test_q_table_update()
        test_reward_calculation()
        test_model_save_load()
        test_episode_management()
        run_simulation()
        
        end_time = time.time()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
        print(f"â±ï¸ ì‹¤í–‰ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
        print(f"ğŸ“… í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
    except AssertionError as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False
    except Exception as e:
        print(f"\nğŸ’¥ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        return False
    
    return True

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1) 