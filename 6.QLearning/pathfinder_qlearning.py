"""
pathfinder_qlearning.py - íŒ¨ìŠ¤íŒŒì¸ë” í‚¤íŠ¸ìš© Q-Learning ììœ¨ì£¼í–‰
ì´ˆìŒíŒŒ ì„¼ì„œë¥¼ ì´ìš©í•œ ì¥ì• ë¬¼ íšŒí”¼ í•™ìŠµ
"""

import time
import random
import numpy as np
import json
import os
from typing import List, Dict, Tuple, Optional
import sys
from pathlib import Path

# ëª¨í„° ì»¨íŠ¸ë¡¤ëŸ¬ì™€ ì´ˆìŒíŒŒ ì„¼ì„œ import
sys.path.append(str(Path(__file__).parent.parent))
from MotorClassTest.Motor import MotorController
from ComponentClass._2.UltrasonicClass.Ultrasonic import UltrasonicSensor

class PathfinderQLearning:
    """íŒ¨ìŠ¤íŒŒì¸ë” í‚¤íŠ¸ìš© Q-Learning ì—ì´ì „íŠ¸"""
    
    def __init__(self, config: Optional[dict] = None):
        """
        Q-Learning ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        
        Args:
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬ (ì„ íƒì‚¬í•­)
        """
        # í•˜ë“œì›¨ì–´ ì´ˆê¸°í™”
        self.motor = MotorController()
        self.ultrasonic = UltrasonicSensor()
        
        # ì„¤ì • ë¡œë“œ
        self.config = self._load_config(config)
        
        # Q-Learning ë³€ìˆ˜
        self.q_table: Dict[str, List[float]] = {}
        self.current_state = ""
        self.last_action = 0
        self.episode = 0
        self.steps = 0
        self.total_reward = 0
        self.running = False
        
        # í†µê³„ ë³€ìˆ˜
        self.episode_rewards = []
        self.episode_steps = []
        
        # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.config['models_dir'], exist_ok=True)
        
        print("ğŸ¤– íŒ¨ìŠ¤íŒŒì¸ë” Q-Learning ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ!")
        print(f"ì•¡ì…˜ ìˆ˜: {len(self.config['actions'])}")
        print(f"ìƒíƒœ ê³µê°„: {self.config['sensor_bins']} bins")
    
    def _load_config(self, config: Optional[dict]) -> dict:
        """ì„¤ì • ë¡œë“œ ë° ê²€ì¦"""
        default_config = {
            # Q-Learning íŒŒë¼ë¯¸í„°
            'learning_rate': 0.1,        # í•™ìŠµë¥  (alpha)
            'discount_factor': 0.95,     # í• ì¸ ì¸ìˆ˜ (gamma)
            'exploration_rate': 1.0,     # ì´ˆê¸° íƒí—˜ë¥  (epsilon)
            'exploration_min': 0.01,     # ìµœì†Œ íƒí—˜ë¥ 
            'exploration_decay': 0.995,  # íƒí—˜ë¥  ê°ì†Œìœ¨
            
            # í™˜ê²½ ì„¤ì •
            'sensor_max': 100,           # ìµœëŒ€ ì„¼ì„œ ê°’ (cm)
            'sensor_bins': 5,            # ì„¼ì„œ ê°’ êµ¬ê°„ ìˆ˜
            'safe_distance': 20,         # ì•ˆì „ ê±°ë¦¬ (cm)
            'warning_distance': 30,      # ê²½ê³  ê±°ë¦¬ (cm)
            'good_distance': 50,         # ì¢‹ì€ ê±°ë¦¬ (cm)
            
            # ëª¨í„° ì„¤ì •
            'base_speed': 40,            # ê¸°ë³¸ ì†ë„
            'turn_speed': 60,            # íšŒì „ ì†ë„
            'slow_speed': 25,            # ëŠë¦° ì†ë„
            
            # ì•¡ì…˜ ì •ì˜ (left_speed, right_speed)
            'actions': [
                (40, 40),    # 0: ì§ì§„
                (25, 40),    # 1: ì•½ê°„ ì¢ŒíšŒì „
                (40, 25),    # 2: ì•½ê°„ ìš°íšŒì „
                (0, 60),     # 3: ê°•í•œ ì¢ŒíšŒì „
                (60, 0),     # 4: ê°•í•œ ìš°íšŒì „
                (-30, 30),   # 5: ì œìë¦¬ ìš°íšŒì „
                (30, -30),   # 6: ì œìë¦¬ ì¢ŒíšŒì „
                (0, 0),      # 7: ì •ì§€
            ],
            
            # ë³´ìƒ ì„¤ì •
            'rewards': {
                'collision': -100,       # ì¶©ëŒ (ë§¤ìš° ê°€ê¹Œì›€)
                'danger': -10,           # ìœ„í—˜ (ê°€ê¹Œì›€)
                'warning': -2,           # ê²½ê³  (ì¡°ê¸ˆ ê°€ê¹Œì›€)
                'normal': 1,             # ì •ìƒ (ì ë‹¹í•œ ê±°ë¦¬)
                'good': 5,               # ì¢‹ìŒ (ì¶©ë¶„í•œ ê±°ë¦¬)
                'step_penalty': -0.1,    # ìŠ¤í… í˜ë„í‹°
            },
            
            # íŒŒì¼ ì„¤ì •
            'models_dir': '6.QLearning/models',
            'model_prefix': 'pathfinder_qtable_',
            'save_interval': 50,         # ëª¨ë¸ ì €ì¥ ê°„ê²©
        }
        
        if config:
            default_config.update(config)
        
        return default_config
    
    def get_distance(self) -> float:
        """ì´ˆìŒíŒŒ ì„¼ì„œë¡œ ê±°ë¦¬ ì¸¡ì •"""
        try:
            distance = self.ultrasonic.get_distance()
            # ì„¼ì„œ ê°’ ì œí•œ
            return min(distance, self.config['sensor_max'])
        except Exception as e:
            print(f"ì„¼ì„œ ì½ê¸° ì˜¤ë¥˜: {e}")
            return self.config['sensor_max']  # ì•ˆì „í•œ ê¸°ë³¸ê°’
    
    def discretize_distance(self, distance: float) -> int:
        """ê±°ë¦¬ë¥¼ ì´ì‚°í™”ëœ ìƒíƒœë¡œ ë³€í™˜"""
        if distance <= 0:
            return 0
        elif distance >= self.config['sensor_max']:
            return self.config['sensor_bins'] - 1
        else:
            # ê±°ë¦¬ë¥¼ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
            bin_size = self.config['sensor_max'] / self.config['sensor_bins']
            return min(int(distance / bin_size), self.config['sensor_bins'] - 1)
    
    def get_state(self) -> str:
        """í˜„ì¬ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°"""
        distance = self.get_distance()
        distance_bin = self.discretize_distance(distance)
        
        # ìƒíƒœë¥¼ ë¬¸ìì—´ë¡œ í‘œí˜„
        state = f"d{distance_bin}"
        return state
    
    def choose_action(self, state: str) -> int:
        """Îµ-greedy ì •ì±…ìœ¼ë¡œ ì•¡ì…˜ ì„ íƒ"""
        # ìƒíƒœê°€ Q-tableì— ì—†ìœ¼ë©´ ì´ˆê¸°í™”
        if state not in self.q_table:
            self.q_table[state] = [0.0] * len(self.config['actions'])
        
        # íƒí—˜ vs í™œìš©
        if random.random() < self.config['exploration_rate']:
            # íƒí—˜: ëœë¤ ì•¡ì…˜
            return random.randint(0, len(self.config['actions']) - 1)
        else:
            # í™œìš©: ìµœê³  Qê°’ ì•¡ì…˜
            return int(np.argmax(self.q_table[state]))
    
    def execute_action(self, action_idx: int) -> None:
        """ì•¡ì…˜ ì‹¤í–‰"""
        left_speed, right_speed = self.config['actions'][action_idx]
        
        try:
            if left_speed == 0 and right_speed == 0:
                self.motor.stop_motors()
            else:
                self.motor.set_individual_speeds(abs(right_speed), abs(left_speed))
                
                # í›„ì§„ì´ í•„ìš”í•œ ê²½ìš° ë°©í–¥ ì„¤ì •
                if left_speed < 0 and right_speed < 0:
                    self.motor.set_motor_direction("left", "backward")
                    self.motor.set_motor_direction("right", "backward")
                elif left_speed < 0:
                    self.motor.set_motor_direction("left", "backward")
                    self.motor.set_motor_direction("right", "forward")
                elif right_speed < 0:
                    self.motor.set_motor_direction("left", "forward")
                    self.motor.set_motor_direction("right", "backward")
                else:
                    self.motor.set_motor_direction("left", "forward")
                    self.motor.set_motor_direction("right", "forward")
                    
        except Exception as e:
            print(f"ì•¡ì…˜ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            self.motor.stop_motors()
    
    def get_reward(self, distance: float, action_idx: int) -> float:
        """ë³´ìƒ ê³„ì‚°"""
        rewards = self.config['rewards']
        
        # ê±°ë¦¬ ê¸°ë°˜ ë³´ìƒ
        if distance < 10:  # ì¶©ëŒ ìœ„í—˜
            return rewards['collision']
        elif distance < self.config['safe_distance']:  # ìœ„í—˜
            return rewards['danger']
        elif distance < self.config['warning_distance']:  # ê²½ê³ 
            return rewards['warning']
        elif distance > self.config['good_distance']:  # ì¢‹ìŒ
            return rewards['good']
        else:  # ì •ìƒ
            return rewards['normal']
    
    def update_q_value(self, state: str, action: int, reward: float, next_state: str) -> None:
        """Q-value ì—…ë°ì´íŠ¸ (Q-learning ê³µì‹)"""
        # ìƒíƒœê°€ Q-tableì— ì—†ìœ¼ë©´ ì´ˆê¸°í™”
        if state not in self.q_table:
            self.q_table[state] = [0.0] * len(self.config['actions'])
        if next_state not in self.q_table:
            self.q_table[next_state] = [0.0] * len(self.config['actions'])
        
        # Q-learning ì—…ë°ì´íŠ¸ ê³µì‹
        # Q(s,a) = Q(s,a) + Î±[r + Î³*max(Q(s',a')) - Q(s,a)]
        best_next_action = np.argmax(self.q_table[next_state])
        td_target = reward + self.config['discount_factor'] * self.q_table[next_state][best_next_action]
        td_error = td_target - self.q_table[state][action]
        self.q_table[state][action] += self.config['learning_rate'] * td_error
    
    def decay_exploration(self) -> None:
        """íƒí—˜ë¥  ê°ì†Œ"""
        self.config['exploration_rate'] = max(
            self.config['exploration_min'],
            self.config['exploration_rate'] * self.config['exploration_decay']
        )
    
    def train_episode(self, max_steps: int = 500) -> float:
        """í•œ ì—í”¼ì†Œë“œ í›ˆë ¨"""
        self.episode += 1
        self.steps = 0
        episode_reward = 0
        
        print(f"\nğŸ¯ ì—í”¼ì†Œë“œ {self.episode} ì‹œì‘")
        
        # ì´ˆê¸° ìƒíƒœ
        self.current_state = self.get_state()
        
        for step in range(max_steps):
            if not self.running:
                break
            
            # ì•¡ì…˜ ì„ íƒ ë° ì‹¤í–‰
            action = self.choose_action(self.current_state)
            self.execute_action(action)
            
            # ì ì‹œ ëŒ€ê¸° (ì•¡ì…˜ ì‹¤í–‰ ì‹œê°„)
            time.sleep(0.2)
            
            # ë‹¤ìŒ ìƒíƒœ ë° ë³´ìƒ ê´€ì°°
            distance = self.get_distance()
            next_state = self.get_state()
            reward = self.get_reward(distance, action)
            
            # Q-value ì—…ë°ì´íŠ¸
            self.update_q_value(self.current_state, action, reward, next_state)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            episode_reward += reward
            self.steps += 1
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if step % 20 == 0:
                action_name = self._get_action_name(action)
                print(f"  ìŠ¤í… {step}: ê±°ë¦¬={distance:.1f}cm, ì•¡ì…˜={action_name}, "
                      f"ë³´ìƒ={reward:.1f}, Îµ={self.config['exploration_rate']:.3f}")
            
            # ì¶©ëŒ ê°ì§€ (ì—í”¼ì†Œë“œ ì¢…ë£Œ)
            if distance < 10:
                print(f"  âš ï¸ ì¶©ëŒ ìœ„í—˜! ì—í”¼ì†Œë“œ ì¢…ë£Œ (ê±°ë¦¬: {distance:.1f}cm)")
                break
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            self.current_state = next_state
        
        # ì—í”¼ì†Œë“œ ì¢…ë£Œ
        self.motor.stop_motors()
        self.decay_exploration()
        
        # í†µê³„ ì €ì¥
        self.episode_rewards.append(episode_reward)
        self.episode_steps.append(self.steps)
        
        print(f"âœ… ì—í”¼ì†Œë“œ {self.episode} ì™„ë£Œ: ë³´ìƒ={episode_reward:.1f}, ìŠ¤í…={self.steps}")
        
        return episode_reward
    
    def _get_action_name(self, action_idx: int) -> str:
        """ì•¡ì…˜ ì¸ë±ìŠ¤ë¥¼ ì´ë¦„ìœ¼ë¡œ ë³€í™˜"""
        action_names = [
            "ì§ì§„", "ì•½ê°„ì¢Œ", "ì•½ê°„ìš°", "ê°•í•œì¢Œ", "ê°•í•œìš°", "ì œìë¦¬ìš°", "ì œìë¦¬ì¢Œ", "ì •ì§€"
        ]
        return action_names[action_idx] if action_idx < len(action_names) else f"ì•¡ì…˜{action_idx}"
    
    def start_training(self, episodes: int = 1000) -> None:
        """í›ˆë ¨ ì‹œì‘"""
        print(f"ğŸš€ Q-Learning í›ˆë ¨ ì‹œì‘! ({episodes} ì—í”¼ì†Œë“œ)")
        print("Ctrl+Cë¡œ ì¤‘ë‹¨ ê°€ëŠ¥")
        
        self.running = True
        
        try:
            for episode in range(episodes):
                if not self.running:
                    break
                
                # ì—í”¼ì†Œë“œ í›ˆë ¨
                episode_reward = self.train_episode()
                
                # ì£¼ê¸°ì ìœ¼ë¡œ ëª¨ë¸ ì €ì¥
                if episode % self.config['save_interval'] == 0:
                    self.save_model()
                
                # í†µê³„ ì¶œë ¥
                if episode % 10 == 0:
                    avg_reward = np.mean(self.episode_rewards[-10:])
                    avg_steps = np.mean(self.episode_steps[-10:])
                    print(f"\nğŸ“Š ìµœê·¼ 10 ì—í”¼ì†Œë“œ í‰ê· : ë³´ìƒ={avg_reward:.1f}, ìŠ¤í…={avg_steps:.1f}")
                    print(f"Q-table í¬ê¸°: {len(self.q_table)} ìƒíƒœ")
                
                # ì—í”¼ì†Œë“œ ê°„ íœ´ì‹
                time.sleep(1)
                
        except KeyboardInterrupt:
            print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ í›ˆë ¨ ì¤‘ë‹¨")
        
        finally:
            self.stop_training()
    
    def stop_training(self) -> None:
        """í›ˆë ¨ ì¤‘ë‹¨"""
        self.running = False
        self.motor.stop_motors()
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        final_path = self.save_model("final_model.json")
        print(f"ğŸ‰ í›ˆë ¨ ì™„ë£Œ! ìµœì¢… ëª¨ë¸ ì €ì¥: {final_path}")
        
        # í†µê³„ ì¶œë ¥
        if self.episode_rewards:
            print(f"\nğŸ“ˆ í›ˆë ¨ í†µê³„:")
            print(f"  ì´ ì—í”¼ì†Œë“œ: {len(self.episode_rewards)}")
            print(f"  í‰ê·  ë³´ìƒ: {np.mean(self.episode_rewards):.2f}")
            print(f"  ìµœê³  ë³´ìƒ: {np.max(self.episode_rewards):.2f}")
            print(f"  í‰ê·  ìŠ¤í…: {np.mean(self.episode_steps):.1f}")
            print(f"  Q-table í¬ê¸°: {len(self.q_table)} ìƒíƒœ")
    
    def save_model(self, filename: Optional[str] = None) -> str:
        """ëª¨ë¸ ì €ì¥"""
        if filename is None:
            filename = f"{self.config['model_prefix']}{self.episode:06d}.json"
        
        filepath = os.path.join(self.config['models_dir'], filename)
        
        # ì €ì¥í•  ë°ì´í„° ì¤€ë¹„
        data = {
            'q_table': {state: q_values for state, q_values in self.q_table.items()},
            'episode': self.episode,
            'config': self.config,
            'statistics': {
                'episode_rewards': self.episode_rewards,
                'episode_steps': self.episode_steps,
            },
            'timestamp': time.time()
        }
        
        try:
            with open(filepath, 'w') as f:
                json.dump(data, f, indent=2)
            print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {filepath}")
            return filepath
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            return ""
    
    def load_model(self, filepath: str) -> bool:
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            with open(filepath, 'r') as f:
                data = json.load(f)
            
            # Q-table ë¡œë“œ
            self.q_table = data['q_table']
            
            # ì„¤ì • ì—…ë°ì´íŠ¸
            if 'config' in data:
                self.config.update(data['config'])
            
            # í†µê³„ ë¡œë“œ
            if 'statistics' in data:
                self.episode_rewards = data['statistics'].get('episode_rewards', [])
                self.episode_steps = data['statistics'].get('episode_steps', [])
            
            if 'episode' in data:
                self.episode = data['episode']
            
            print(f"ğŸ“‚ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {filepath}")
            print(f"  Q-table í¬ê¸°: {len(self.q_table)} ìƒíƒœ")
            print(f"  ì—í”¼ì†Œë“œ: {self.episode}")
            return True
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def run_trained_model(self, max_steps: int = 1000) -> None:
        """í›ˆë ¨ëœ ëª¨ë¸ë¡œ ì‹¤í–‰ (íƒí—˜ ì—†ì´)"""
        print("ğŸ® í›ˆë ¨ëœ ëª¨ë¸ë¡œ ì‹¤í–‰ ì‹œì‘!")
        print("Ctrl+Cë¡œ ì¤‘ë‹¨")
        
        # íƒí—˜ë¥ ì„ 0ìœ¼ë¡œ ì„¤ì • (ìˆœìˆ˜ í™œìš©)
        original_exploration = self.config['exploration_rate']
        self.config['exploration_rate'] = 0.0
        
        try:
            step = 0
            while step < max_steps:
                # í˜„ì¬ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
                state = self.get_state()
                distance = self.get_distance()
                
                # ìµœì  ì•¡ì…˜ ì„ íƒ
                action = self.choose_action(state)
                action_name = self._get_action_name(action)
                
                # ì•¡ì…˜ ì‹¤í–‰
                self.execute_action(action)
                
                # ìƒíƒœ ì¶œë ¥
                if step % 10 == 0:
                    print(f"ìŠ¤í… {step}: ê±°ë¦¬={distance:.1f}cm, ì•¡ì…˜={action_name}")
                
                # ì¶©ëŒ ê°ì§€
                if distance < 15:
                    print(f"âš ï¸ ì¥ì• ë¬¼ ê°ì§€! ê±°ë¦¬: {distance:.1f}cm")
                
                step += 1
                time.sleep(0.2)
                
        except KeyboardInterrupt:
            print("\nâ¹ï¸ ì‹¤í–‰ ì¤‘ë‹¨")
        
        finally:
            self.motor.stop_motors()
            self.config['exploration_rate'] = original_exploration
            print("ğŸ ì‹¤í–‰ ì™„ë£Œ")
    
    def cleanup(self) -> None:
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.motor.stop_motors()
        self.motor.cleanup()
        print("ğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ¤– íŒ¨ìŠ¤íŒŒì¸ë” Q-Learning ììœ¨ì£¼í–‰")
    print("=" * 50)
    
    # Q-Learning ì—ì´ì „íŠ¸ ìƒì„±
    agent = PathfinderQLearning()
    
    try:
        while True:
            print("\në©”ë‰´:")
            print("1. ìƒˆë¡œìš´ í›ˆë ¨ ì‹œì‘")
            print("2. ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ í›„ í›ˆë ¨ ê³„ì†")
            print("3. í›ˆë ¨ëœ ëª¨ë¸ë¡œ ì‹¤í–‰")
            print("4. ì¢…ë£Œ")
            
            choice = input("ì„ íƒí•˜ì„¸ìš” (1-4): ").strip()
            
            if choice == '1':
                episodes = int(input("í›ˆë ¨í•  ì—í”¼ì†Œë“œ ìˆ˜ (ê¸°ë³¸ 100): ") or "100")
                agent.start_training(episodes)
                
            elif choice == '2':
                model_path = input("ëª¨ë¸ íŒŒì¼ ê²½ë¡œ: ").strip()
                if agent.load_model(model_path):
                    episodes = int(input("ì¶”ê°€ í›ˆë ¨í•  ì—í”¼ì†Œë“œ ìˆ˜ (ê¸°ë³¸ 100): ") or "100")
                    agent.start_training(episodes)
                
            elif choice == '3':
                model_path = input("ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸: 6.QLearning/models/final_model.json): ").strip()
                if not model_path:
                    model_path = "6.QLearning/models/final_model.json"
                
                if agent.load_model(model_path):
                    max_steps = int(input("ìµœëŒ€ ì‹¤í–‰ ìŠ¤í… ìˆ˜ (ê¸°ë³¸ 1000): ") or "1000")
                    agent.run_trained_model(max_steps)
                
            elif choice == '4':
                break
                
            else:
                print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
    
    except KeyboardInterrupt:
        print("\ní”„ë¡œê·¸ë¨ ì¤‘ë‹¨")
    
    finally:
        agent.cleanup()


if __name__ == "__main__":
    main() 