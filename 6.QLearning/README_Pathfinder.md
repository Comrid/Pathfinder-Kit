# ğŸ¤– íŒ¨ìŠ¤íŒŒì¸ë” Q-Learning ììœ¨ì£¼í–‰

íŒ¨ìŠ¤íŒŒì¸ë” í‚¤íŠ¸ë¥¼ ìœ„í•œ ê°•í™”í•™ìŠµ ê¸°ë°˜ ììœ¨ì£¼í–‰ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. Q-Learning ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ì´ˆìŒíŒŒ ì„¼ì„œë¡œ ì¥ì• ë¬¼ì„ íšŒí”¼í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨
- [Q-Learningì´ë€?](#q-learningì´ë€)
- [ì‹œìŠ¤í…œ êµ¬ì„±](#ì‹œìŠ¤í…œ-êµ¬ì„±)
- [ì‚¬ìš©ë²•](#ì‚¬ìš©ë²•)
- [ì›¹ ì¸í„°í˜ì´ìŠ¤](#ì›¹-ì¸í„°í˜ì´ìŠ¤)
- [íŒŒë¼ë¯¸í„° ì„¤ì •](#íŒŒë¼ë¯¸í„°-ì„¤ì •)
- [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)

## ğŸ§  Q-Learningì´ë€?

### ê°•í™”í•™ìŠµì˜ ê¸°ë³¸ ê°œë…
- **ì—ì´ì „íŠ¸(Agent)**: í•™ìŠµí•˜ëŠ” ì£¼ì²´ (íŒ¨ìŠ¤íŒŒì¸ë” ë¡œë´‡)
- **í™˜ê²½(Environment)**: ë¡œë´‡ì´ ì›€ì§ì´ëŠ” ê³µê°„
- **ìƒíƒœ(State)**: í˜„ì¬ ìƒí™© (ì´ˆìŒíŒŒ ì„¼ì„œ ê±°ë¦¬)
- **ì•¡ì…˜(Action)**: ì·¨í•  ìˆ˜ ìˆëŠ” í–‰ë™ (ì§ì§„, ì¢ŒíšŒì „, ìš°íšŒì „ ë“±)
- **ë³´ìƒ(Reward)**: í–‰ë™ì— ëŒ€í•œ í”¼ë“œë°± (ì¢‹ìŒ/ë‚˜ì¨)

### Q-Learning ê³µì‹
```
Q(s,a) = Q(s,a) + Î±[r + Î³*max(Q(s',a')) - Q(s,a)]
```
- **Q(s,a)**: ìƒíƒœ sì—ì„œ ì•¡ì…˜ aì˜ ê°€ì¹˜
- **Î±**: í•™ìŠµë¥  (0.1)
- **Î³**: í• ì¸ ì¸ìˆ˜ (0.95)
- **r**: ì¦‰ì‹œ ë³´ìƒ
- **s'**: ë‹¤ìŒ ìƒíƒœ

## ğŸ”§ ì‹œìŠ¤í…œ êµ¬ì„±

### í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­
- Raspberry Pi Zero 2 W
- ì´ˆìŒíŒŒ ì„¼ì„œ HC-SR04
- ëª¨í„° ë“œë¼ì´ë²„ L298N
- DC ê¸°ì–´ ëª¨í„° 2ê°œ

### ì†Œí”„íŠ¸ì›¨ì–´ êµ¬ì„±
```
6.QLearning/
â”œâ”€â”€ pathfinder_qlearning.py    # ë©”ì¸ Q-Learning êµ¬í˜„
â”œâ”€â”€ qlearning_web.py           # ì›¹ ì¸í„°í˜ì´ìŠ¤
â”œâ”€â”€ models/                    # ì €ì¥ëœ ëª¨ë¸ë“¤
â””â”€â”€ README_Pathfinder.md       # ì´ íŒŒì¼
```

## ğŸš€ ì‚¬ìš©ë²•

### 1. í„°ë¯¸ë„ ëª¨ë“œ (ê¸°ë³¸)

```bash
cd 6.QLearning
python pathfinder_qlearning.py
```

**ë©”ë‰´ ì˜µì…˜:**
1. **ìƒˆë¡œìš´ í›ˆë ¨ ì‹œì‘**: ì²˜ìŒë¶€í„° í•™ìŠµ ì‹œì‘
2. **ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ í›„ í›ˆë ¨ ê³„ì†**: ì €ì¥ëœ ëª¨ë¸ì—ì„œ ì´ì–´ì„œ í•™ìŠµ
3. **í›ˆë ¨ëœ ëª¨ë¸ë¡œ ì‹¤í–‰**: í•™ìŠµ ì™„ë£Œëœ ëª¨ë¸ë¡œ ììœ¨ì£¼í–‰
4. **ì¢…ë£Œ**: í”„ë¡œê·¸ë¨ ì¢…ë£Œ

### 2. ì›¹ ì¸í„°í˜ì´ìŠ¤ ëª¨ë“œ (ì¶”ì²œ)

```bash
cd 6.QLearning
python qlearning_web.py
```

ë¸Œë¼ìš°ì €ì—ì„œ `http://ë¼ì¦ˆë² ë¦¬íŒŒì´IP:5000` ì ‘ì†

## ğŸŒ ì›¹ ì¸í„°í˜ì´ìŠ¤

### ì£¼ìš” ê¸°ëŠ¥

#### ğŸ® í›ˆë ¨ ì œì–´ íŒ¨ë„
- **í›ˆë ¨ ì‹œì‘/ì¤‘ë‹¨**: ì‹¤ì‹œê°„ í›ˆë ¨ ì œì–´
- **ì—í”¼ì†Œë“œ ìˆ˜ ì„¤ì •**: í•™ìŠµí•  ì—í”¼ì†Œë“œ ê°œìˆ˜
- **ìµœëŒ€ ìŠ¤í… ì„¤ì •**: ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ í–‰ë™ ìˆ˜

#### ğŸ“Š ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
- **í˜„ì¬ ì—í”¼ì†Œë“œ/ìŠ¤í…**: í•™ìŠµ ì§„í–‰ ìƒí™©
- **í˜„ì¬ ë³´ìƒ**: ì‹¤ì‹œê°„ ë³´ìƒ ê°’
- **ê±°ë¦¬**: ì´ˆìŒíŒŒ ì„¼ì„œ ì¸¡ì •ê°’
- **íƒí—˜ë¥ **: í˜„ì¬ Îµ(epsilon) ê°’
- **Q-Table í¬ê¸°**: í•™ìŠµëœ ìƒíƒœ ìˆ˜

#### âš™ï¸ íŒŒë¼ë¯¸í„° ì¡°ì •
- **í•™ìŠµë¥  (Î±)**: ì–¼ë§ˆë‚˜ ë¹¨ë¦¬ í•™ìŠµí• ì§€
- **í• ì¸ ì¸ìˆ˜ (Î³)**: ë¯¸ë˜ ë³´ìƒì˜ ì¤‘ìš”ë„
- **íƒí—˜ë¥  ê°ì†Œ**: Îµ ê°’ ê°ì†Œ ì†ë„
- **ì•ˆì „ ê±°ë¦¬**: ì¥ì• ë¬¼ íšŒí”¼ ê¸°ì¤€ ê±°ë¦¬

#### ğŸ“ˆ í•™ìŠµ ì§„í–‰ ì°¨íŠ¸
- **ì—í”¼ì†Œë“œë³„ ë³´ìƒ**: ì‹¤ì‹œê°„ í•™ìŠµ ì„±ê³¼
- **í‰ê·  ë³´ìƒ**: ìµœê·¼ 10 ì—í”¼ì†Œë“œ í‰ê· 
- **í•™ìŠµ ê³¡ì„ **: ì‹œê°„ì— ë”°ë¥¸ ì„±ëŠ¥ í–¥ìƒ

#### ğŸ’¾ ëª¨ë¸ ê´€ë¦¬
- **ëª¨ë¸ ì €ì¥/ë¡œë“œ**: í•™ìŠµ ê²°ê³¼ ì €ì¥ ë° ë¶ˆëŸ¬ì˜¤ê¸°
- **ëª¨ë¸ ëª©ë¡**: ì €ì¥ëœ ëª¨ë¸ íŒŒì¼ ê´€ë¦¬
- **ìë™ ì €ì¥**: 50 ì—í”¼ì†Œë“œë§ˆë‹¤ ìë™ ì €ì¥

## âš™ï¸ íŒŒë¼ë¯¸í„° ì„¤ì •

### Q-Learning íŒŒë¼ë¯¸í„°

#### í•™ìŠµë¥  (Learning Rate, Î±)
```python
learning_rate = 0.1  # ê¸°ë³¸ê°’
```
- **ë†’ìŒ (0.5-1.0)**: ë¹ ë¥¸ í•™ìŠµ, ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŒ
- **ì¤‘ê°„ (0.1-0.3)**: ê· í˜•ì¡íŒ í•™ìŠµ (ê¶Œì¥)
- **ë‚®ìŒ (0.01-0.1)**: ì•ˆì •ì ì´ì§€ë§Œ ëŠë¦° í•™ìŠµ

#### í• ì¸ ì¸ìˆ˜ (Discount Factor, Î³)
```python
discount_factor = 0.95  # ê¸°ë³¸ê°’
```
- **ë†’ìŒ (0.9-0.99)**: ì¥ê¸°ì  ë³´ìƒ ì¤‘ì‹œ
- **ë‚®ìŒ (0.1-0.5)**: ì¦‰ì‹œ ë³´ìƒ ì¤‘ì‹œ

#### íƒí—˜ë¥  (Exploration Rate, Îµ)
```python
exploration_rate = 1.0      # ì´ˆê¸°ê°’ (100% íƒí—˜)
exploration_min = 0.01      # ìµœì†Œê°’ (1% íƒí—˜)
exploration_decay = 0.995   # ê°ì†Œìœ¨
```

### í™˜ê²½ ì„¤ì •

#### ê±°ë¦¬ ê¸°ì¤€
```python
safe_distance = 20      # ì•ˆì „ ê±°ë¦¬ (cm)
warning_distance = 30   # ê²½ê³  ê±°ë¦¬ (cm)
good_distance = 50      # ì¢‹ì€ ê±°ë¦¬ (cm)
```

#### ë³´ìƒ ì‹œìŠ¤í…œ
```python
rewards = {
    'collision': -100,    # ì¶©ëŒ ìœ„í—˜ (10cm ë¯¸ë§Œ)
    'danger': -10,        # ìœ„í—˜ (20cm ë¯¸ë§Œ)
    'warning': -2,        # ê²½ê³  (30cm ë¯¸ë§Œ)
    'normal': 1,          # ì •ìƒ (30-50cm)
    'good': 5,            # ì¢‹ìŒ (50cm ì´ìƒ)
}
```

### ì•¡ì…˜ ì •ì˜
```python
actions = [
    (40, 40),    # 0: ì§ì§„
    (25, 40),    # 1: ì•½ê°„ ì¢ŒíšŒì „
    (40, 25),    # 2: ì•½ê°„ ìš°íšŒì „
    (0, 60),     # 3: ê°•í•œ ì¢ŒíšŒì „
    (60, 0),     # 4: ê°•í•œ ìš°íšŒì „
    (-30, 30),   # 5: ì œìë¦¬ ìš°íšŒì „
    (30, -30),   # 6: ì œìë¦¬ ì¢ŒíšŒì „
    (0, 0),      # 7: ì •ì§€
]
```

## ğŸ“Š í•™ìŠµ ê³¼ì • ì´í•´

### í•™ìŠµ ë‹¨ê³„

#### 1ë‹¨ê³„: íƒí—˜ (Exploration)
- **íŠ¹ì§•**: ëœë¤í•œ í–‰ë™, ë†’ì€ Îµ ê°’
- **ëª©ì **: í™˜ê²½ íƒìƒ‰, ë‹¤ì–‘í•œ ê²½í—˜ ìˆ˜ì§‘
- **ê¸°ê°„**: ì´ˆê¸° 100-200 ì—í”¼ì†Œë“œ

#### 2ë‹¨ê³„: í•™ìŠµ (Learning)
- **íŠ¹ì§•**: ì ì§„ì  Îµ ê°ì†Œ, Q-table êµ¬ì¶•
- **ëª©ì **: ìƒíƒœ-ì•¡ì…˜ ê°€ì¹˜ í•™ìŠµ
- **ê¸°ê°„**: 200-500 ì—í”¼ì†Œë“œ

#### 3ë‹¨ê³„: í™œìš© (Exploitation)
- **íŠ¹ì§•**: ë‚®ì€ Îµ ê°’, í•™ìŠµëœ ì •ì±… ì‚¬ìš©
- **ëª©ì **: ìµœì  í–‰ë™ ìˆ˜í–‰
- **ê¸°ê°„**: 500+ ì—í”¼ì†Œë“œ

### ì„±ëŠ¥ ì§€í‘œ

#### ì—í”¼ì†Œë“œ ë³´ìƒ
- **ì´ˆê¸°**: -50 ~ -20 (ë§ì€ ì¶©ëŒ)
- **ì¤‘ê¸°**: -10 ~ 10 (ì ì§„ì  ê°œì„ )
- **í›„ê¸°**: 10 ~ 50 (ì•ˆì •ì  íšŒí”¼)

#### Q-Table í¬ê¸°
- **ì´ˆê¸°**: 1-5 ìƒíƒœ
- **ì¤‘ê¸°**: 5-15 ìƒíƒœ
- **í›„ê¸°**: 10-20 ìƒíƒœ (ìˆ˜ë ´)

## ğŸ”§ ê³ ê¸‰ ì„¤ì •

### ìƒíƒœ ê³µê°„ í™•ì¥
```python
# í˜„ì¬: ê±°ë¦¬ë§Œ ì‚¬ìš©
state = f"d{distance_bin}"

# í™•ì¥ ì˜ˆì‹œ: ì†ë„ ì¶”ê°€
state = f"d{distance_bin}_v{velocity_bin}"

# í™•ì¥ ì˜ˆì‹œ: ë°©í–¥ ì¶”ê°€
state = f"d{distance_bin}_dir{direction_bin}"
```

### ë³´ìƒ í•¨ìˆ˜ ì»¤ìŠ¤í„°ë§ˆì´ì§•
```python
def custom_reward(self, distance, action_idx):
    """ì‚¬ìš©ì ì •ì˜ ë³´ìƒ í•¨ìˆ˜"""
    base_reward = self.get_reward(distance, action_idx)
    
    # ì§ì§„ ë³´ë„ˆìŠ¤
    if action_idx == 0 and distance > 30:
        base_reward += 2
    
    # ê¸‰íšŒì „ í˜ë„í‹°
    if action_idx in [3, 4, 5, 6]:
        base_reward -= 1
    
    return base_reward
```

### ë‹¤ì¤‘ ì„¼ì„œ ì§€ì›
```python
def get_multi_sensor_state(self):
    """ë‹¤ì¤‘ ì„¼ì„œ ìƒíƒœ"""
    front_distance = self.ultrasonic_front.get_distance()
    left_distance = self.ultrasonic_left.get_distance()
    right_distance = self.ultrasonic_right.get_distance()
    
    front_bin = self.discretize_distance(front_distance)
    left_bin = self.discretize_distance(left_distance)
    right_bin = self.discretize_distance(right_distance)
    
    return f"f{front_bin}_l{left_bin}_r{right_bin}"
```

## ğŸ› ë¬¸ì œ í•´ê²°

### í•™ìŠµì´ ì•ˆ ë  ë•Œ

#### ì¦ìƒ: ë³´ìƒì´ ê³„ì† ìŒìˆ˜
**ì›ì¸**: í•™ìŠµë¥ ì´ ë„ˆë¬´ ë‚®ê±°ë‚˜ íƒí—˜ì´ ë¶€ì¡±
**í•´ê²°ì±…**:
```python
learning_rate = 0.2        # ì¦ê°€
exploration_decay = 0.99   # ëŠë¦° ê°ì†Œ
```

#### ì¦ìƒ: ê°™ì€ í–‰ë™ë§Œ ë°˜ë³µ
**ì›ì¸**: íƒí—˜ë¥ ì´ ë„ˆë¬´ ë¹¨ë¦¬ ê°ì†Œ
**í•´ê²°ì±…**:
```python
exploration_min = 0.1      # ìµœì†Œê°’ ì¦ê°€
exploration_decay = 0.999  # ë” ëŠë¦° ê°ì†Œ
```

### í•˜ë“œì›¨ì–´ ë¬¸ì œ

#### ì„¼ì„œ ì½ê¸° ì˜¤ë¥˜
```python
# ì„¼ì„œ ì•ˆì •í™”
def get_stable_distance(self, samples=3):
    distances = []
    for _ in range(samples):
        distances.append(self.ultrasonic.get_distance())
        time.sleep(0.01)
    return np.median(distances)
```

#### ëª¨í„° ì œì–´ ë¶ˆì•ˆì •
```python
# ë¶€ë“œëŸ¬ìš´ ì†ë„ ë³€í™”
def smooth_speed_change(self, target_left, target_right):
    current_left = self.current_left_speed
    current_right = self.current_right_speed
    
    # ì ì§„ì  ì†ë„ ë³€í™”
    for step in range(5):
        left = current_left + (target_left - current_left) * (step + 1) / 5
        right = current_right + (target_right - current_right) * (step + 1) / 5
        self.motor.set_individual_speeds(int(right), int(left))
        time.sleep(0.02)
```

### ì„±ëŠ¥ ìµœì í™”

#### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¤„ì´ê¸°
```python
# Q-table í¬ê¸° ì œí•œ
if len(self.q_table) > 1000:
    # ì˜¤ë˜ëœ ìƒíƒœ ì œê±°
    old_states = list(self.q_table.keys())[:100]
    for state in old_states:
        del self.q_table[state]
```

#### í•™ìŠµ ì†ë„ í–¥ìƒ
```python
# ë°°ì¹˜ ì—…ë°ì´íŠ¸
def batch_update(self, experiences):
    for state, action, reward, next_state in experiences:
        self.update_q_value(state, action, reward, next_state)
```

## ğŸ“š ì¶”ê°€ í•™ìŠµ ìë£Œ

### ê°•í™”í•™ìŠµ ì´ë¡ 
- [Sutton & Barto - Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book.html)
- [OpenAI Spinning Up](https://spinningup.openai.com/)

### ì‹¤ìŠµ ì˜ˆì œ
- [Q-Learning ì‹œê°í™”](https://cs.stanford.edu/people/karpathy/reinforcejs/)
- [GridWorld ì˜ˆì œ](https://github.com/dennybritz/reinforcement-learning)

### ê³ ê¸‰ ì•Œê³ ë¦¬ì¦˜
- **Deep Q-Network (DQN)**: ì‹ ê²½ë§ ê¸°ë°˜ Q-Learning
- **Policy Gradient**: ì •ì±… ê¸°ë°˜ í•™ìŠµ
- **Actor-Critic**: ê°€ì¹˜ + ì •ì±… ê²°í•©

## ğŸ¯ í”„ë¡œì íŠ¸ í™•ì¥ ì•„ì´ë””ì–´

### 1. ë‹¤ì¤‘ ëª©í‘œ í•™ìŠµ
- ì¥ì• ë¬¼ íšŒí”¼ + ëª©í‘œ ì§€ì  ë„ë‹¬
- ì—ë„ˆì§€ íš¨ìœ¨ì„± ìµœì í™”
- ì‹œê°„ ìµœì†Œí™” ê²½ë¡œ íƒìƒ‰

### 2. í™˜ê²½ ë³µì¡ë„ ì¦ê°€
- ë™ì  ì¥ì• ë¬¼ (ì›€ì§ì´ëŠ” ë¬¼ì²´)
- ë‹¤ì–‘í•œ ì§€í˜• (ì¹´í«, íƒ€ì¼ ë“±)
- ì¡°ëª… ë³€í™” ëŒ€ì‘

### 3. ì„¼ì„œ ìœµí•©
- ì¹´ë©”ë¼ + ì´ˆìŒíŒŒ ì„¼ì„œ
- IMU ì„¼ì„œ ì¶”ê°€
- ë¼ì´ë‹¤ ì„¼ì„œ ì—°ë™

### 4. í˜‘ë ¥ í•™ìŠµ
- ë‹¤ì¤‘ ë¡œë´‡ í˜‘ë ¥
- í†µì‹  ê¸°ë°˜ ì •ë³´ ê³µìœ 
- ì§‘ë‹¨ ì§€ëŠ¥ êµ¬í˜„

---

ğŸ‰ **Q-Learningìœ¼ë¡œ ë˜‘ë˜‘í•œ ììœ¨ì£¼í–‰ ë¡œë´‡ì„ ë§Œë“¤ì–´ë³´ì„¸ìš”!** 